{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ViCoQA_GraphFlow.ipynb","provenance":[],"collapsed_sections":["YCwBHzCRyxsy","RTjZBfBfznRW","IdQQQounz16e","KdL57a1Dy14P","QHrI4kW0zZlv","il_GNyJzwJsH"],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1Xz_WKUANv9n71yZy44nYuBw6TMV4sHFa","authorship_tag":"ABX9TyMFlg6C/PQCo6J5XIdu7sMQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f-RNfRiErygD"},"source":["https://github.com/hugochan/GraphFlow"]},{"cell_type":"code","metadata":{"id":"p_ckLzzNr7iW","executionInfo":{"status":"ok","timestamp":1610352565358,"user_tz":-420,"elapsed":853,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["TRAIN = 'drive/MyDrive/CODE/CMRC/dataset/vicoqa-train.json'\n","DEV = 'drive/MyDrive/CODE/CMRC/dataset/vicoqa-dev.json'\n","TEST = 'drive/MyDrive/CODE/CMRC/dataset/vicoqa-test.json'\n","\n","TRAIN_PREPROCESSED = 'drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-train.json'\n","DEV_PREPROCESSED = 'drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-dev.json'\n","TEST_PREPROCESSED = 'drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-test.json'\n","\n","EMBEDDING = 'drive/MyDrive/CODE/CMRC/embedding/wiki.vi.vec'\n","RC_MODEL = 'drive/MyDrive/CODE/CMRC/data_graphflow/models2/'"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"fepDYZJqrYLm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610352637928,"user_tz":-420,"elapsed":73413,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}},"outputId":"6d4739d7-8fc6-45d5-8070-5e2280b81b45"},"source":["pip install torch==0.4.1"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting torch==0.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n","\u001b[K     |████████████████████████████████| 519.5MB 30kB/s \n","\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fastai 1.0.61 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: torch\n","  Found existing installation: torch 1.7.0+cu101\n","    Uninstalling torch-1.7.0+cu101:\n","      Successfully uninstalled torch-1.7.0+cu101\n","Successfully installed torch-0.4.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P3yIH9VYsldI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610352642432,"user_tz":-420,"elapsed":77914,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}},"outputId":"5076cd01-7612-4814-abea-c77d37ab7a41"},"source":["pip install PyYAML==5.1"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting PyYAML==5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n","\r\u001b[K     |█▏                              | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 28.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 19.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 16.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 15.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81kB 15.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 16.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 13.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: PyYAML\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.1-cp36-cp36m-linux_x86_64.whl size=44075 sha256=4b8ac3635c53e5b503622f40408b50a5bb2020fc59f8d5be8047d37b6fb0dfd3\n","  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n","Successfully built PyYAML\n","\u001b[31mERROR: fastai 1.0.61 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n","Installing collected packages: PyYAML\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeJPO-hYF9O8","executionInfo":{"status":"ok","timestamp":1610352646820,"user_tz":-420,"elapsed":82299,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}},"outputId":"fee0a4db-295b-4bc5-b591-fe9eb47a778f"},"source":["pip install pyvi"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting pyvi\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/e1/0e5bc6b5e3327b9385d6e0f1b0a7c0404f28b74eb6db59a778515b30fd9c/pyvi-0.1-py2.py3-none-any.whl (8.5MB)\n","\u001b[K     |████████████████████████████████| 8.5MB 13.2MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyvi) (0.22.2.post1)\n","Collecting sklearn-crfsuite\n","  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.0.0)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.19.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (0.8.7)\n","Collecting python-crfsuite>=0.8.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n","\u001b[K     |████████████████████████████████| 747kB 61.5MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n","Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n","Successfully installed python-crfsuite-0.9.7 pyvi-0.1 sklearn-crfsuite-0.3.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e4IV-WBps_jR"},"source":["# Config"]},{"cell_type":"code","metadata":{"id":"KCZkFg_btCVS","executionInfo":{"status":"ok","timestamp":1610352646822,"user_tz":-420,"elapsed":82299,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["config = {\n","  \"dataset_name\": \"coqa\",\n","  \"trainset\": TRAIN_PREPROCESSED,\n","  \"devset\": DEV_PREPROCESSED,\n","  \"testset\": TEST_PREPROCESSED,\n","  \"embed_file\": EMBEDDING,\n","  \"saved_vocab_file\": RC_MODEL + \"word_model_min_5\",\n","  \"pretrained\": None,\n","  \"out_dir\": RC_MODEL,\n","  \"min_freq\": 5,\n","  \"top_vocab\": 200000,\n","  \"n_history\": 5,\n","  \"no_pre_question\": False,\n","  \"no_pre_answer\": False,\n","  \"max_turn_num\": 50,\n","  \"embed_type\": \"fasttext\",\n","  \"vocab_embed_size\": 300,\n","  \"fix_vocab_embed\": True,\n","  \"f_qem\": True,\n","  \"f_pos\": False,\n","  \"f_ner\": False,\n","  \"f_tf\": False,\n","  \"ctx_exact_match_embed_dim\": 3,\n","  \"ctx_pos_embed_dim\": 12,\n","  \"ctx_ner_embed_dim\": 8,\n","  \"answer_marker_embed_dim\": 10,\n","  \"use_ques_marker\": True,\n","  \"ques_marker_embed_dim\": 3,\n","  \"ques_turn_marker_embed_dim\": 5,\n","  \"hidden_size\": 300,\n","  \"word_dropout\": 0.3,\n","  \"bert_dropout\": 0.4,\n","  \"rnn_dropout\": 0.3,\n","  \"rnn_input_dropout\": None,\n","  \"use_gnn\": True,\n","  \"bignn\": False,\n","  \"static_graph\": False,\n","  \"temporal_gnn\": True,\n","  \"ctx_graph_hops\": 5,\n","  \"ctx_graph_topk\": 10,\n","  \"graph_learner_num_pers\": 1,\n","  \"use_spatial_kernels\": False,\n","  \"n_spatial_kernels\": 2,\n","  \"use_position_enc\": False,\n","  \"max_position_distance\": 160,\n","  \"position_emb_size\": 50,\n","  \"use_bert\": False,\n","  \"finetune_bert\": False,\n","  \"use_bert_weight\": True,\n","  \"use_bert_gamma\": False,\n","  \"bert_model\": \"bert-large-uncased\",\n","  \"bert_dim\": 1024,\n","  \"bert_max_seq_len\": 500,\n","  \"bert_doc_stride\": 250,\n","  \"bert_layer_indexes\": [\n","    0,\n","    24\n","  ],\n","  \"optimizer\": \"adamax\",\n","  \"learning_rate\": 0.0005,\n","  \"grad_clipping\": 10,\n","  \"random_seed\": 1234,\n","  \"shuffle\": True,\n","  \"batch_size\": 1,\n","  \"grad_accumulated_steps\": 1,\n","  \"test_batch_size\": 1,\n","  \"max_epochs\": 30,\n","  \"patience\": 10,\n","  \"verbose\": 1000,\n","  \"max_answer_len\": 12,\n","  \"predict_train\": True,\n","  \"out_predictions\": True,\n","  \"predict_raw_text\": True,\n","  \"save_params\": True,\n","  \"logging\": True,\n","  \"out_pred_in_folder\": True,\n","  \"no_cuda\": False,\n","  \"cuda_id\": 0\n","}"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2-eTZFvxriX","executionInfo":{"status":"ok","timestamp":1610352646822,"user_tz":-420,"elapsed":82299,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["# GENERAL :\n","VERY_SMALL_NUMBER = 1e-12\n","\n","_UNK_POS = 'unk_pos'\n","_UNK_NER = 'unk_ner'\n","\n","_UNK_TOKEN = '<<unk>>'\n","_QUESTION_SYMBOL = '<q>'\n","_ANSWER_SYMBOL = '<a>'\n","\n","\n","# CoQA :\n","CoQA_UNK_ANSWER = 'unknown'\n","CoQA_YES_ANSWER = 'yes'\n","CoQA_NO_ANSWER = 'no'\n","\n","CoQA_UNK_ANSWER_LABEL = 0\n","CoQA_ANSWER_YES_LABEL = 1\n","CoQA_ANSWER_NO_LABEL = 2\n","CoQA_ANSWER_SPAN_LABEL = 3\n","CoQA_ANSWER_CLASS_NUM = 4\n","\n","\n","# QuAC\n","QuAC_UNK_ANSWER = 'cannotanswer'\n","\n","QuAC_YESNO_YES = 'y'\n","QuAC_YESNO_NO = 'n'\n","QuAC_YESNO_OTHER = 'x'\n","\n","QuAC_YESNO_YES_LABEL = 0\n","QuAC_YESNO_NO_LABEL = 1\n","QuAC_YESNO_OTHER_LABEL = 2\n","QuAC_YESNO_CLASS_NUM = 3\n","\n","QuAC_FOLLOWUP_YES = 'y'\n","QuAC_FOLLOWUP_NO = 'n'\n","QuAC_FOLLOWUP_OTHER = 'm'\n","\n","QuAC_FOLLOWUP_YES_LABEL = 0\n","QuAC_FOLLOWUP_NO_LABEL = 1\n","QuAC_FOLLOWUP_OTHER_LABEL = 2\n","QuAC_FOLLOWUP_CLASS_NUM = 3\n","\n","# LOG FILES ##\n","\n","_CONFIG_FILE = \"config.json\"\n","_SAVED_WEIGHTS_FILE = \"params.saved\"\n","_PREDICTION_FILE = \"pred.json\""],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExOsrskkuiag"},"source":["# Pre-process"]},{"cell_type":"code","metadata":{"id":"QrEwqbcPuleA","executionInfo":{"status":"ok","timestamp":1610352647727,"user_tz":-420,"elapsed":83202,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["import string\n","import json\n","import pickle\n","import collections\n","import random\n","import os\n","import time\n","import codecs\n","import sys\n","import re\n","import io\n","from tqdm import tqdm\n","import time\n","from collections import Counter, defaultdict\n","from pyvi import ViTokenizer\n","\n","def get_str(output, lower=False):\n","    s = \" \".join(output['word'])\n","    return s.lower() if lower else s\n","\n","def _get_str(input):\n","    s = ' '.join(input)\n","    return s\n","\n","# Create a list of token of a string\n","def tokenize(sent):\n","    tokens = ViTokenizer.tokenize(sent).split(\" \")\n","    tokens = [w.lower() for w in tokens]\n","    return tokens\n","\n","def find_span(offsets, start, end):\n","    start_index = end_index = -1\n","    for i, offset in enumerate(offsets):\n","        if (start_index < 0) or (start >= offset[0]):\n","            start_index = i\n","        if (end_index < 0) and (end <= offset[1]):\n","            end_index = i\n","    return (start_index, end_index)\n","\n","# Get position of all token in story\n","def offsets_process(text, tokens):\n","    txt = text.lower()\n","    tokens = [i.replace(\"_\", \" \") for i in tokens]\n","    token_ids = []\n","    cur_id = 0\n","    for i, token in enumerate(tokens):\n","        start = txt.find(token, cur_id)\n","        token_ids.append((start, start + len(token)))\n","        cur_id = start + len(token)\n","    return token_ids\n","\n","def process(text):\n","    output = {'word': [],\n","                'offsets': []}\n","    list_tokens = tokenize(text)\n","    list_offsets = offsets_process(text, list_tokens)\n","    output[\"word\"] = list_tokens\n","    output[\"offsets\"] = list_offsets\n","    return output\n","\n","def getList(dict): \n","    return dict.keys()\n","\n","def preprocess_data(data_file, output_file1):\n","    with open(data_file, 'r', encoding=\"utf8\") as f:\n","        dataset = json.load(f)\n","\n","    data = []\n","    start_time = time.time()\n","    for i, datum in enumerate(dataset['data']):\n","        if i % 10 == 0:\n","            print('processing %d / %d (used_time = %.2fs)...' % (i, len(dataset['data']), time.time() - start_time))\n","        context_str = datum['story']\n","        _datum = {'context': context_str,\n","                'source': datum['source'],\n","                'id': str(datum['id']),\n","                'filename': datum['filename']}\n","        _datum['annotated_context'] = process(context_str)\n","        _datum['qas'] = []\n","\n","        assert len(datum['questions']) == len(datum['answers'])\n","\n","        additional_answers = {}\n","        if 'additional_answers' in datum:\n","            for k, answer in datum['additional_answers'].items():\n","                if len(answer) == len(datum['answers']):\n","                    for ex in answer:\n","                        idx = ex['turn_id']\n","                        if idx not in additional_answers:\n","                            additional_answers[idx] = []\n","                            additional_answers[idx].append(ex['input_text'])\n","        \n","        for question, answer in zip(datum['questions'], datum['answers']):\n","            assert question['turn_id'] == answer['turn_id']\n","            idx = question['turn_id']\n","            _qas = {'turn_id': idx, \n","                    'question': question['input_text'],\n","                    'answer': answer['input_text']}\n","            if idx in additional_answers:\n","                _qas['additional_answers'] = additional_answers[idx]\n","\n","            _qas['annotated_question'] = process(question['input_text'])\n","            _qas['annotated_answer'] = process(answer['input_text'])\n","            _qas['answer_span_start'] = answer['span_start']\n","            _qas['answer_span_end'] = answer['span_end']\n","            start = answer['span_start']\n","            end = answer['span_end']\n","\n","            chosen_text = _datum['context'][start: end].lower()\n","            while len(chosen_text) > 0 and chosen_text[0] in string.whitespace:\n","                chosen_text = chosen_text[1:]\n","                start += 1\n","            while len(chosen_text) > 0 and chosen_text[-1] in string.whitespace:\n","                chosen_text = chosen_text[:-1]\n","                end -= 1\n","            s = 0\n","            e = 0\n","            input_text = _qas['answer'].strip().lower()\n","            if input_text in chosen_text:\n","                i = chosen_text.find(input_text)\n","                _qas['answer_span'] = find_span(_datum['annotated_context']['offsets'],\n","                                                        start + i, start + i + len(input_text))\n","                s = start + i\n","                e = s + len(input_text)\n","            else:\n","                _qas['answer_span'] = find_span(_datum['annotated_context']['offsets'], start, end)\n","                s = start\n","                e = end\n","            _datum['qas'].append(_qas)\n","          \n","        data.append(_datum)\n","\n","    dataset['data'] = data\n","    with open(output_file1, 'w', encoding=\"utf8\") as output_file:\n","        json.dump(dataset, output_file, ensure_ascii=False)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckMJMjXpvZ0r","executionInfo":{"status":"ok","timestamp":1610352647729,"user_tz":-420,"elapsed":83203,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["# preprocess_data(TRAIN, TRAIN_PREPROCESSED)\n","# preprocess_data(DEV, DEV_PREPROCESSED)\n","# preprocess_data(TEST, TEST_PREPROCESSED)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bynpYutEsp_N"},"source":["# Model "]},{"cell_type":"markdown","metadata":{"id":"Hw9Qe-AIwmey"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"oUuqYa3own5d","executionInfo":{"status":"ok","timestamp":1610352653677,"user_tz":-420,"elapsed":89149,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["import json\n","import io\n","import torch\n","import numpy as np\n","from scipy.sparse import *\n","from collections import Counter, defaultdict, namedtuple\n","\n","from torch.utils.data import Dataset\n","import re\n","import string\n","import time\n","\n","import os\n","import sys\n","\n","import torch\n","\n","\n","# from .bert_utils import *\n","# from .eval_utils import normalize_text\n","# from . import constants as Constants\n","# from .timer import Timer\n","\n","def extract_bert_ques_hidden_states(all_encoder_layers, max_doc_len, features, weighted_avg=False):\n","  num_layers, batch_size, turn_size, num_chunk, max_token_len, bert_dim = all_encoder_layers.shape\n","  out_features = torch.Tensor(num_layers, batch_size, turn_size, max_doc_len, bert_dim).fill_(0)\n","  device = all_encoder_layers.get_device() if all_encoder_layers.is_cuda else None\n","  if device is not None:\n","    out_features = out_features.to(device)\n","\n","  token_count = []\n","  # Map BERT tokens to doc words\n","  for i, ex_feature in enumerate(features): # Example\n","    ex_token_count = []\n","    for t, para_feature in enumerate(ex_feature): # Turn\n","      para_token_count = defaultdict(int)\n","      for j, chunk_feature in enumerate(para_feature): # Chunk\n","        for k in chunk_feature.token_is_max_context: # Token\n","          if chunk_feature.token_is_max_context[k]:\n","            doc_word_idx = chunk_feature.token_to_orig_map[k]\n","            out_features[:, i, t, doc_word_idx] += all_encoder_layers[:, i, t, j, k]\n","            para_token_count[doc_word_idx] += 1\n","      ex_token_count.append(para_token_count)\n","    token_count.append(ex_token_count)\n","\n","  for i, ex_token_count in enumerate(token_count):\n","    for t, para_token_count in enumerate(ex_token_count):\n","      for doc_word_idx, count in para_token_count.items():\n","        out_features[:, i, t, doc_word_idx] /= count\n","\n","  # Average through all layers\n","  if not weighted_avg:\n","    out_features = torch.mean(out_features, 0)\n","  return out_features\n","\n","def extract_bert_ctx_hidden_states(all_encoder_layers, max_doc_len, features, weighted_avg=False):\n","  num_layers, batch_size, num_chunk, max_token_len, bert_dim = all_encoder_layers.shape\n","  out_features = torch.Tensor(num_layers, batch_size, max_doc_len, bert_dim).fill_(0)\n","  device = all_encoder_layers.get_device() if all_encoder_layers.is_cuda else None\n","  if device is not None:\n","    out_features = out_features.to(device)\n","\n","  token_count = []\n","  # Map BERT tokens to doc words\n","  for i, ex_feature in enumerate(features): # Example\n","    ex_token_count = defaultdict(int)\n","    for j, chunk_feature in enumerate(ex_feature): # Chunk\n","      for k in chunk_feature.token_is_max_context: # Token\n","        if chunk_feature.token_is_max_context[k]:\n","          doc_word_idx = chunk_feature.token_to_orig_map[k]\n","          out_features[:, i, doc_word_idx] += all_encoder_layers[:, i, j, k]\n","          ex_token_count[doc_word_idx] += 1\n","    token_count.append(ex_token_count)\n","\n","  for i, ex_token_count in enumerate(token_count):\n","    for doc_word_idx, count in ex_token_count.items():\n","      out_features[:, i, doc_word_idx] /= count\n","\n","  # Average through all layers\n","  if not weighted_avg:\n","    out_features = torch.mean(out_features, 0)\n","  return out_features\n","\n","def convert_text_to_bert_features(text, bert_tokenizer, max_seq_length, doc_stride):\n","  # The convention in BERT is:\n","      # (a) For sequence pairs:\n","      #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","      #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n","      # (b) For single sequences:\n","      #  tokens:   [CLS] the dog is hairy . [SEP]\n","      #  type_ids: 0   0   0   0  0     0 0\n","\n","  tok_to_orig_index = []\n","  all_doc_tokens = []\n","  for (i, token) in enumerate(text):\n","    sub_tokens = bert_tokenizer.wordpiece_tokenizer.tokenize(token.lower())\n","    for sub_ in sub_tokens:\n","      tok_to_orig_index.append(i)\n","      all_doc_tokens.append(sub_)\n","\n","  # The -2 accounts for [CLS] and [SEP]\n","  max_tokens_for_doc = max_seq_length - 2\n","\n","  # We can have documents that are longer than the maximum sequence length.\n","  # To deal with this we do a sliding window approach, where we take chunks\n","  # of the up to our max length with a stride of `doc_stride`.\n","  _DocSpan = namedtuple(  # pylint: disable=invalid-name\n","      \"DocSpan\", [\"start\", \"length\"])\n","  doc_spans = []\n","  start_offset = 0\n","  while start_offset < len(all_doc_tokens):\n","    length = len(all_doc_tokens) - start_offset\n","    if length > max_tokens_for_doc:\n","      length = max_tokens_for_doc\n","    doc_spans.append(_DocSpan(start=start_offset, length=length))\n","    if start_offset + length == len(all_doc_tokens):\n","      break\n","    start_offset += min(length, doc_stride)\n","\n","  out_features = []\n","  for (doc_span_index, doc_span) in enumerate(doc_spans):\n","    tokens = []\n","    token_to_orig_map = {}\n","    token_is_max_context = {}\n","    segment_ids = []\n","    tokens.append(\"[CLS]\")\n","    segment_ids.append(0)\n","\n","    for i in range(doc_span.length):\n","      split_token_index = doc_span.start + i\n","      token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n","\n","      is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n","                                             split_token_index)\n","      token_is_max_context[len(tokens)] = is_max_context\n","      tokens.append(all_doc_tokens[split_token_index])\n","      segment_ids.append(0)\n","    tokens.append(\"[SEP]\")\n","    segment_ids.append(0)\n","\n","    input_ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n","\n","    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","    # tokens are attended to.\n","    input_mask = [1] * len(input_ids)\n","\n","    feature = BertInputFeatures(\n","    doc_span_index=doc_span_index,\n","    tokens=tokens,\n","    token_to_orig_map=token_to_orig_map,\n","    token_is_max_context=token_is_max_context,\n","    input_ids=input_ids,\n","    input_mask=input_mask,\n","    segment_ids=segment_ids)\n","    out_features.append(feature)\n","  return out_features\n","\n","\n","def _check_is_max_context(doc_spans, cur_span_index, position):\n","  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n","\n","  # Because of the sliding window approach taken to scoring documents, a single\n","  # token can appear in multiple documents. E.g.\n","  #  Doc: the man went to the store and bought a gallon of milk\n","  #  Span A: the man went to the\n","  #  Span B: to the store and bought\n","  #  Span C: and bought a gallon of\n","  #  ...\n","  #\n","  # Now the word 'bought' will have two scores from spans B and C. We only\n","  # want to consider the score with \"maximum context\", which we define as\n","  # the *minimum* of its left and right context (the *sum* of left and\n","  # right context will always be the same, of course).\n","  #\n","  # In the example the maximum context for 'bought' would be span C since\n","  # it has 1 left context and 3 right context, while span B has 4 left context\n","  # and 0 right context.\n","  best_score = None\n","  best_span_index = None\n","  for (span_index, doc_span) in enumerate(doc_spans):\n","    end = doc_span.start + doc_span.length - 1\n","    if position < doc_span.start:\n","      continue\n","    if position > end:\n","      continue\n","    num_left_context = position - doc_span.start\n","    num_right_context = end - position\n","    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n","    if best_score is None or score > best_score:\n","      best_score = score\n","      best_span_index = span_index\n","\n","  return cur_span_index == best_span_index\n","\n","class BertInputFeatures(object):\n","    \"\"\"A single set of BERT features of data.\"\"\"\n","\n","    def __init__(self,\n","                 doc_span_index,\n","                 tokens,\n","                 token_to_orig_map,\n","                 token_is_max_context,\n","                 input_ids,\n","                 input_mask,\n","                 segment_ids):\n","        self.doc_span_index = doc_span_index\n","        self.tokens = tokens\n","        self.token_to_orig_map = token_to_orig_map\n","        self.token_is_max_context = token_is_max_context\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","\n","################################################################################\n","# Text Processing Helper Functions #\n","################################################################################\n","\n","\n","def normalize_text(s):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    def remove_articles(text):\n","        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value.\"\"\"\n","    def __init__(self):\n","        self.history = []\n","        self.last = None\n","        self.val = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def reset(self):\n","        self.last = self.mean()\n","        self.history.append(self.last)\n","        self.val = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","\n","    def mean(self):\n","        if self.count == 0:\n","            return 0.\n","        return self.sum / self.count\n","\n","\n","################################################################################\n","# Dataset Prep #\n","################################################################################\n","\n","def prepare_datasets(config):\n","    train_set = None if config['trainset'] is None else QADataset(config['trainset'], config)\n","    dev_set = None if config['devset'] is None else QADataset(config['devset'], config)\n","    test_set = None if config['testset'] is None else QADataset(config['testset'], config)\n","    return {'train': train_set, 'dev': dev_set, 'test': test_set}\n","\n","################################################################################\n","# Dataset Classes #\n","################################################################################\n","\n","class QADataset(Dataset):\n","    \"\"\"QA dataset.\"\"\"\n","\n","    def __init__(self, filename, config):\n","        timer = Timer('Load %s' % filename)\n","        self.filename = filename\n","        self.config = config\n","        paragraph_lens = []\n","        question_lens = []\n","        turn_num = []\n","        self.paragraphs = []\n","        self.vocab = Counter()\n","        dataset = read_json(filename)\n","        for paragraph in dataset['data']: # Paragraph level\n","            paragraph_lens.append(len(paragraph['annotated_context']['word']))\n","            turn_num.append(len(paragraph['qas']))\n","            # Prepare paragraphs\n","            history = []\n","            para = {'turns': []}\n","            for qas in paragraph['qas']: # Turn level\n","                # Build vocab\n","                for w in qas['annotated_question']['word'] \\\n","                        + paragraph['annotated_context']['word'] \\\n","                        + qas['annotated_answer']['word']:\n","                    self.vocab[w.lower()] += 1\n","\n","                temp = []\n","                marker = []\n","                n_history = len(history) if self.config['n_history'] < 0 else min(self.config['n_history'], len(history))\n","                if n_history > 0:\n","                    count = sum([not config['no_pre_question'], not config['no_pre_answer']]) * len(history[-n_history:])\n","                    for q, a in history[-n_history:]:\n","                        if not config['no_pre_question']:\n","                            temp.extend(q)\n","                            marker.extend([count] * len(q))\n","                            count -= 1\n","                        if not config['no_pre_answer']:\n","                            temp.extend(a)\n","                            marker.extend([count] * len(a))\n","                            count -= 1\n","                temp.extend(qas['annotated_question']['word'])\n","                marker.extend([0] * len(qas['annotated_question']['word']))\n","                history.append((qas['annotated_question']['word'], qas['annotated_answer']['word']))\n","                qas['annotated_question']['word'] = temp\n","                qas['annotated_question']['marker'] = marker\n","                question_lens.append(len(qas['annotated_question']['word']))\n","\n","                # Prepare a question-answer pair\n","                question = qas['annotated_question']\n","\n","                if config['dataset_name'] == 'coqa':\n","                    answers = [qas['answer']]\n","                    if 'additional_answers' in qas:\n","                        answers = answers + qas['additional_answers']\n","                else:\n","                    answers = qas['additional_answers']\n","\n","                normalized_answer = normalize_text(qas['answer'])\n","                sample = {'turn_id': qas['turn_id'],\n","                          'question': question,\n","                          'answers': answers,\n","                          'targets': qas['answer_span'],\n","                          'span_mask': 0}\n","\n","                if config['dataset_name'] == 'coqa':\n","                    if CoQA_UNK_ANSWER == normalized_answer:\n","                        sample['answer_type_targets'] = CoQA_UNK_ANSWER_LABEL\n","                    elif CoQA_YES_ANSWER == normalized_answer:\n","                        sample['answer_type_targets'] = CoQA_ANSWER_YES_LABEL\n","                    elif CoQA_NO_ANSWER == normalized_answer:\n","                        sample['answer_type_targets'] = CoQA_ANSWER_NO_LABEL\n","                    else:\n","                        sample['answer_type_targets'] = CoQA_ANSWER_SPAN_LABEL\n","                        sample['span_mask'] = 1\n","\n","                else:\n","                    if QuAC_UNK_ANSWER == normalized_answer:\n","                        sample['unk_answer_targets'] = 1\n","                    else:\n","                        sample['unk_answer_targets'] = 0\n","                        sample['span_mask'] = 1\n","\n","                    if qas['yesno'] == QuAC_YESNO_YES:\n","                        sample['yesno_targets'] = QuAC_YESNO_YES_LABEL\n","                    elif qas['yesno'] == QuAC_YESNO_NO:\n","                        sample['yesno_targets'] = QuAC_YESNO_NO_LABEL\n","                    else:\n","                        sample['yesno_targets'] = QuAC_YESNO_OTHER_LABEL\n","\n","                    if qas['followup'] == QuAC_FOLLOWUP_YES:\n","                        sample['followup_targets'] = QuAC_FOLLOWUP_YES_LABEL\n","                    elif qas['followup'] == QuAC_FOLLOWUP_NO:\n","                        sample['followup_targets'] = QuAC_FOLLOWUP_NO_LABEL\n","                    else:\n","                        sample['followup_targets'] = QuAC_FOLLOWUP_OTHER_LABEL\n","\n","                para['id'] = paragraph['id']\n","                para['evidence'] = paragraph['annotated_context']\n","\n","                if self.config['predict_raw_text']:\n","                    para['raw_evidence'] = paragraph['context']\n","                para['turns'].append(sample)\n","            self.paragraphs.append(para)\n","        print('Load {} paragraphs.'.format(len(self.paragraphs)))\n","        print('Turn num: avg = %.1f, min = %d, max = %d' % (np.average(turn_num), np.min(turn_num), np.max(turn_num)))\n","        print('Paragraph length: avg = %.1f, min = %d, max = %d' % (np.average(paragraph_lens), np.min(paragraph_lens), np.max(paragraph_lens)))\n","        print('Question length: avg = %.1f, min = %d, max = %d' % (np.average(question_lens), np.min(question_lens), np.max(question_lens)))\n","        timer.finish()\n","\n","    def __len__(self):\n","        return len(self.paragraphs)\n","\n","    def __getitem__(self, idx):\n","        return self.paragraphs[idx]\n","\n","################################################################################\n","# Read & Write Helper Functions #\n","################################################################################\n","\n","\n","def write_json_to_file(json_object, json_file, mode='w', encoding='utf-8'):\n","    with io.open(json_file, mode, encoding=encoding) as outfile:\n","        json.dump(json_object, outfile, indent=4, sort_keys=True, ensure_ascii=False)\n","\n","\n","def log_json(data, filename, mode='w', encoding='utf-8'):\n","    with io.open(filename, mode, encoding=encoding) as outfile:\n","        outfile.write(json.dumps(data, indent=4, ensure_ascii=False))\n","\n","\n","def get_file_contents(filename, encoding='utf-8'):\n","    with io.open(filename, encoding=encoding) as f:\n","        content = f.read()\n","    f.close()\n","    return content\n","\n","\n","def read_json(filename, encoding='utf-8'):\n","    contents = get_file_contents(filename, encoding=encoding)\n","    return json.loads(contents)\n","\n","\n","def get_processed_file_contents(file_path, encoding=\"utf-8\"):\n","    contents = get_file_contents(file_path, encoding=encoding)\n","    return contents.strip()\n","\n","################################################################################\n","# DataLoader Helper Functions #\n","################################################################################\n","\n","def sanitize_input(sample_batch, config, vocab, feature_dict, bert_tokenizer, training=True):\n","    \"\"\"\n","    Reformats sample_batch for easy vectorization.\n","    Args:\n","        sample_batch: the sampled batch, yet to be sanitized or vectorized.\n","        vocab: word embedding dictionary.\n","        feature_dict: the features we want to concatenate to our embeddings.\n","        train: train or test?\n","    \"\"\"\n","    sanitized_batch = defaultdict(list)\n","    batch_graphs = []\n","    for paragraph in sample_batch:\n","        if 'id' in paragraph:\n","            sanitized_batch['id'].append(paragraph['id'])\n","        evidence = paragraph['evidence']['word']\n","        processed_e = [vocab[w.lower()] if w.lower() in vocab else vocab[_UNK_TOKEN] for w in evidence]\n","        sanitized_batch['evidence'].append(processed_e)\n","\n","        if config.get('static_graph', None):\n","            batch_graphs.append(paragraph['evidence']['graph'])\n","\n","        if config['f_tf']:\n","            sanitized_batch['evidence_tf'].append(compute_tf(evidence))\n","\n","        if config['predict_raw_text']:\n","            sanitized_batch['raw_evidence_text'].append(paragraph['raw_evidence'])\n","            sanitized_batch['offsets'].append(paragraph['evidence']['offsets'])\n","        else:\n","            sanitized_batch['evidence_text'].append(evidence)\n","\n","        para_turn_ids = []\n","        para_ques = []\n","        para_ques_marker = []\n","        para_bert_ques_features = []\n","        para_features = []\n","        para_targets = []\n","        para_span_mask = []\n","        if config['dataset_name'] == 'coqa':\n","            para_answer_type_targets = []\n","        else:\n","            para_unk_answer_targets = []\n","            para_yesno_targets = []\n","            para_followup_targets = []\n","\n","        para_answers = []\n","        for ex in paragraph['turns']:\n","            para_turn_ids.append(ex['turn_id'])\n","            question = ex['question']['word']\n","            processed_q = [vocab[w.lower()] if w.lower() in vocab else vocab[_UNK_TOKEN] for w in question]\n","            para_ques.append(processed_q)\n","            para_ques_marker.append(ex['question']['marker'])\n","\n","            if config['use_bert']:\n","                bert_ques_features = convert_text_to_bert_features(question, bert_tokenizer, config['bert_max_seq_len'], config['bert_doc_stride'])\n","                para_bert_ques_features.append(bert_ques_features)\n","\n","            # featurize evidence document:\n","            para_features.append(featurize(ex['question'], paragraph['evidence'], feature_dict))\n","            para_targets.append(ex['targets'])\n","            para_span_mask.append(ex['span_mask'])\n","            para_answers.append(ex['answers'])\n","            if config['dataset_name'] == 'coqa':\n","                para_answer_type_targets.append(ex['answer_type_targets'])\n","            else:\n","                para_unk_answer_targets.append(ex['unk_answer_targets'])\n","                para_yesno_targets.append(ex['yesno_targets'])\n","                para_followup_targets.append(ex['followup_targets'])\n","\n","        sanitized_batch['question'].append(para_ques)\n","        sanitized_batch['question_marker'].append(para_ques_marker)\n","        if config['use_bert']:\n","            bert_evidence_features = convert_text_to_bert_features(evidence, bert_tokenizer, config['bert_max_seq_len'], config['bert_doc_stride'])\n","            sanitized_batch['bert_evidence_features'].append(bert_evidence_features)\n","            sanitized_batch['bert_question_features'].append(para_bert_ques_features)\n","\n","        sanitized_batch['turn_ids'].append(para_turn_ids)\n","        sanitized_batch['features'].append(para_features)\n","        sanitized_batch['targets'].append(para_targets)\n","        sanitized_batch['span_mask'].append(para_span_mask)\n","        sanitized_batch['answers'].append(para_answers)\n","\n","        if config['dataset_name'] == 'coqa':\n","            sanitized_batch['answer_type_targets'].append(para_answer_type_targets)\n","        else:\n","            sanitized_batch['unk_answer_targets'].append(para_unk_answer_targets)\n","            sanitized_batch['yesno_targets'].append(para_yesno_targets)\n","            sanitized_batch['followup_targets'].append(para_followup_targets)\n","\n","    if config.get('static_graph', None):\n","        batch_graphs = cons_batch_graph(batch_graphs)\n","        sanitized_batch['evidence_graphs'] = vectorize_batch_graph(batch_graphs)\n","\n","    return sanitized_batch\n","\n","def vectorize_input(batch, config, bert_model, training=True, device=None):\n","    \"\"\"\n","    - Vectorize question and question mask\n","    - Vectorize evidence documents, mask and features\n","    - Vectorize target representations\n","    \"\"\"\n","    # Check there is at least one valid example in batch (containing targets):\n","    if not batch:\n","        return None\n","\n","    # Relevant parameters:\n","    batch_size = len(batch['question'])\n","\n","    # Initialize all relevant parameters to None:\n","    targets = None\n","\n","    # Part 1: Question Words\n","    # Batch questions ( sum_bs(n_sect), len_q)\n","    max_q_len = max([len(q) for para_q in batch['question'] for q in para_q])\n","    max_turn_len = max([len(para_q) for para_q in batch['question']])\n","    xq = torch.LongTensor(batch_size, max_turn_len, max_q_len).fill_(0)\n","    xq_len = torch.LongTensor(batch_size, max_turn_len).fill_(1)\n","    num_turn = torch.LongTensor(batch_size).fill_(0)\n","    if config['use_ques_marker']:\n","        xq_f = torch.LongTensor(batch_size, max_turn_len, max_q_len).fill_(0)\n","\n","    for i, para_q in enumerate(batch['question']):\n","        num_turn[i] = len(para_q)\n","        for j, q in enumerate(para_q):\n","            xq[i, j, :len(q)].copy_(torch.LongTensor(q))\n","            if config['use_ques_marker']:\n","                xq_f[i, j, :len(q)].copy_(torch.LongTensor(batch['question_marker'][i][j]))\n","            xq_len[i, j] = len(q)\n","\n","    # Part 2: Document Words\n","    max_d_len = max([len(d) for d in batch['evidence']])\n","    xd = torch.LongTensor(batch_size, max_d_len).fill_(0)\n","    xd_len = torch.LongTensor(batch_size).fill_(1)\n","\n","\n","    # 2(a): fill up DrQA section variables\n","    if config['f_tf']:\n","        xd_tf = torch.Tensor(batch_size, max_d_len).fill_(0)\n","        for i, d in enumerate(batch['evidence_tf']):\n","            xd_tf[i, :len(d)].copy_(torch.Tensor(d))\n","\n","    xd_f = {}\n","    for i, d in enumerate(batch['evidence']):\n","        xd[i, :len(d)].copy_(torch.LongTensor(d))\n","        xd_len[i] = len(d)\n","        # Context features\n","        for j, para_features in enumerate(batch['features'][i]):\n","            for feat_key, feat_val in para_features.items():\n","                if not feat_key in xd_f:\n","                    xd_f[feat_key] = torch.zeros(batch_size, max_turn_len, max_d_len, dtype=torch.long)\n","                xd_f[feat_key][i, j, :len(d)].copy_(feat_val)\n","\n","    # Part 3: Target representations\n","    targets = torch.LongTensor(batch_size, max_turn_len, 2).fill_(-100)\n","    for i, _target in enumerate(batch['targets']):\n","        for j in range(len(_target)):\n","            targets[i, j, 0] = _target[j][0]\n","            targets[i, j, 1] = _target[j][1]\n","\n","    # Part 4: UNK/YES/NO answer masks\n","    span_mask = torch.Tensor(batch_size, max_turn_len).fill_(0)\n","    for i, _span_mask in enumerate(batch['span_mask']):\n","        for j in range(len(_span_mask)):\n","            span_mask[i, j] = _span_mask[j]\n","\n","    if config['dataset_name'] == 'coqa':\n","        answer_type_targets = torch.LongTensor(batch_size, max_turn_len).fill_(-100)\n","        for i, _unk_yes_no_target in enumerate(batch['answer_type_targets']):\n","            for j in range(len(_unk_yes_no_target)):\n","                answer_type_targets[i, j] = _unk_yes_no_target[j]\n","    else:\n","        unk_answer_targets = torch.Tensor(batch_size, max_turn_len).fill_(-100)\n","        yesno_targets = torch.LongTensor(batch_size, max_turn_len).fill_(-100)\n","        followup_targets = torch.LongTensor(batch_size, max_turn_len).fill_(-100)\n","        for i, _unk_answer_target in enumerate(batch['unk_answer_targets']):\n","            for j in range(len(_unk_answer_target)):\n","                unk_answer_targets[i, j] = _unk_answer_target[j]\n","                yesno_targets[i, j] = batch['yesno_targets'][i][j]\n","                followup_targets[i, j] = batch['followup_targets'][i][j]\n","\n","    # Part 5: Previous answer markers\n","    if config['n_history'] > 0:\n","        if config['answer_marker_embed_dim'] != 0:\n","            xd_answer_marker = torch.LongTensor(batch_size, max_turn_len, max_d_len, config['n_history']).fill_(0)\n","            for i, _target in enumerate(batch['targets']):\n","                for j in range(len(_target)):\n","                    if _target[j][0] > 0 and _target[j][1] > 0:\n","                        for prev_answer_distance in range(config['n_history']):\n","                            turn_id = j + prev_answer_distance + 1\n","                            if turn_id < len(_target):\n","                                mark_prev_answer(_target[j][0], _target[j][1], xd_answer_marker[i, turn_id, :, prev_answer_distance], prev_answer_distance)\n","\n","    # Part 6: Extract features from pretrained BERT models\n","    if config['use_bert']:\n","        with torch.set_grad_enabled(False):\n","            # Question words\n","            max_bert_q_num_chunks = max([len(para_bert_q) for ex_bert_q in batch['bert_question_features'] for para_bert_q in ex_bert_q])\n","            max_bert_q_len = max([len(bert_q.input_ids) for ex_bert_q in batch['bert_question_features'] for para_bert_q in ex_bert_q for bert_q in para_bert_q])\n","            bert_xq = torch.LongTensor(batch_size, max_turn_len, max_bert_q_num_chunks, max_bert_q_len).fill_(0)\n","            bert_xq_mask = torch.LongTensor(batch_size, max_turn_len, max_bert_q_num_chunks, max_bert_q_len).fill_(0)\n","            for i, ex_bert_q in enumerate(batch['bert_question_features']):\n","                for t, para_bert_q in enumerate(ex_bert_q):\n","                    for j, bert_q in enumerate(para_bert_q):\n","                        bert_xq[i, t, j, :len(bert_q.input_ids)].copy_(torch.LongTensor(bert_q.input_ids))\n","                        bert_xq_mask[i, t, j, :len(bert_q.input_mask)].copy_(torch.LongTensor(bert_q.input_mask))\n","            if device:\n","                bert_xq = bert_xq.to(device)\n","                bert_xq_mask = bert_xq_mask.to(device)\n","\n","            layer_indexes = list(range(config['bert_layer_indexes'][0], config['bert_layer_indexes'][1]))\n","            all_encoder_layers, _ = bert_model(bert_xq.view(-1, bert_xq.size(-1)), token_type_ids=None, attention_mask=bert_xq_mask.view(-1, bert_xq_mask.size(-1)))\n","            torch.cuda.empty_cache()\n","            all_encoder_layers = torch.stack([x.view(bert_xq.shape + (-1,)) for x in all_encoder_layers], 0).detach()\n","            all_encoder_layers = all_encoder_layers[layer_indexes]\n","            bert_xq_f = extract_bert_ques_hidden_states(all_encoder_layers, max_q_len, batch['bert_question_features'], weighted_avg=config['use_bert_weight'])\n","            torch.cuda.empty_cache()\n","\n","            # Document words\n","            max_bert_d_num_chunks = max([len(ex_bert_d) for ex_bert_d in batch['bert_evidence_features']])\n","            max_bert_d_len = max([len(bert_d.input_ids) for ex_bert_d in batch['bert_evidence_features'] for bert_d in ex_bert_d])\n","            bert_xd = torch.LongTensor(batch_size, max_bert_d_num_chunks, max_bert_d_len).fill_(0)\n","            bert_xd_mask = torch.LongTensor(batch_size, max_bert_d_num_chunks, max_bert_d_len).fill_(0)\n","            for i, ex_bert_d in enumerate(batch['bert_evidence_features']): # Example level\n","                for j, bert_d in enumerate(ex_bert_d): # Chunk level\n","                    bert_xd[i, j, :len(bert_d.input_ids)].copy_(torch.LongTensor(bert_d.input_ids))\n","                    bert_xd_mask[i, j, :len(bert_d.input_mask)].copy_(torch.LongTensor(bert_d.input_mask))\n","            if device:\n","                bert_xd = bert_xd.to(device)\n","                bert_xd_mask = bert_xd_mask.to(device)\n","            all_encoder_layers, _ = bert_model(bert_xd.view(-1, bert_xd.size(-1)), token_type_ids=None, attention_mask=bert_xd_mask.view(-1, bert_xd_mask.size(-1)))\n","            torch.cuda.empty_cache()\n","            all_encoder_layers = torch.stack([x.view(bert_xd.shape + (-1,)) for x in all_encoder_layers], 0).detach()\n","            all_encoder_layers = all_encoder_layers[layer_indexes]\n","            bert_xd_f = extract_bert_ctx_hidden_states(all_encoder_layers, max_d_len, batch['bert_evidence_features'], weighted_avg=config['use_bert_weight'])\n","            torch.cuda.empty_cache()\n","\n","    with torch.set_grad_enabled(training):\n","        example = {'batch_size': batch_size,\n","                   'answers': batch['answers'],\n","                   'xq': xq.to(device) if device else xq,\n","                   'xq_len': xq_len.to(device) if device else xq_len,\n","                   'xd': xd.to(device) if device else xd,\n","                   'xd_len': xd_len.to(device) if device else xd_len,\n","                   'num_turn': num_turn.to(device) if device else num_turn,\n","                   'targets': targets.to(device) if device else targets,\n","                   'span_mask': span_mask.to(device) if device else span_mask}\n","\n","        if config.get('static_graph', None):\n","            example['xd_graphs'] = batch['evidence_graphs']\n","\n","        if config['f_tf']:\n","            example['xd_tf'] = xd_tf.to(device) if device else xd_tf\n","\n","        if config['dataset_name'] == 'coqa':\n","            example['answer_type_targets'] = answer_type_targets.to(device) if device else answer_type_targets\n","        else:\n","            example['unk_answer_targets'] = unk_answer_targets.to(device) if device else unk_answer_targets\n","            example['yesno_targets'] = yesno_targets.to(device) if device else yesno_targets\n","            example['followup_targets'] = followup_targets.to(device) if device else followup_targets\n","\n","        if config['predict_raw_text']:\n","            example['raw_evidence_text'] = batch['raw_evidence_text']\n","            example['offsets'] = batch['offsets']\n","        else:\n","            example['evidence_text'] = batch['evidence_text']\n","\n","        if config['use_bert']:\n","            example['bert_xq_f'] = bert_xq_f\n","            example['bert_xd_f'] = bert_xd_f\n","\n","        if device:\n","            for feat_key in xd_f:\n","                xd_f[feat_key] = xd_f[feat_key].to(device)\n","        example['xd_f'] = xd_f\n","\n","        if config['n_history'] > 0:\n","            if config['answer_marker_embed_dim'] != 0:\n","                example['xd_answer_marker'] = xd_answer_marker.to(device) if device else xd_answer_marker\n","            if config['use_ques_marker']:\n","                example['xq_f'] = xq_f.to(device) if device else xq_f\n","        return example\n","\n","\n","def featurize(question, document, feature_dict):\n","    doc_len = len(document['word'])\n","    features = {}\n","    if 'f_qem' in feature_dict:\n","        features['f_qem'] = torch.zeros(doc_len, dtype=torch.long)\n","    if 'f_pos' in feature_dict:\n","        features['f_pos'] = torch.zeros(doc_len, dtype=torch.long)\n","    if 'f_ner' in feature_dict:\n","        features['f_ner'] = torch.zeros(doc_len, dtype=torch.long)\n","\n","    q_uncased_words = set([w.lower() for w in question['word']])\n","    for i in range(doc_len):\n","        d_word = document['word'][i]\n","        if 'f_qem' in feature_dict:\n","            features['f_qem'][i] = feature_dict['f_qem']['yes'] if d_word.lower() in q_uncased_words else feature_dict['f_qem']['no']\n","        if 'f_pos' in feature_dict:\n","            assert 'pos' in document\n","            features['f_pos'][i] = feature_dict['f_pos'][document['pos'][i]] if document['pos'][i] in feature_dict['f_pos'] \\\n","                                    else feature_dict['f_pos'][_UNK_POS]\n","        if 'f_ner' in feature_dict:\n","            assert 'ner' in document\n","            features['f_ner'][i] = feature_dict['f_ner'][document['ner'][i]] if document['ner'][i] in feature_dict['f_ner'] \\\n","                                    else feature_dict['f_ner'][_UNK_NER]\n","    return features\n","\n","def mark_prev_answer(span_start, span_end, evidence_answer_marker, prev_answer_distance):\n","    assert prev_answer_distance >= 0\n","    try:\n","        assert span_start >= 0\n","        assert span_end >= 0\n","    except:\n","        raise ValueError(\"Previous {0:d}th answer span should have been updated!\".format(prev_answer_distance))\n","    # Modify \"tags\" to mark previous answer span.\n","    if span_start == span_end:\n","        evidence_answer_marker[span_start] = 4 * prev_answer_distance + 1\n","    else:\n","        evidence_answer_marker[span_start] = 4 * prev_answer_distance + 2\n","        evidence_answer_marker[span_end] = 4 * prev_answer_distance + 3\n","        for passage_index in range(span_start + 1, span_end):\n","            evidence_answer_marker[passage_index] = 4 * prev_answer_distance + 4\n","\n","def compute_tf(doc):\n","    doc_len = float(len(doc))\n","    word_count = Counter(doc)\n","    tf = []\n","    for word in doc:\n","        tf.append(word_count[word] / doc_len)\n","    return tf\n","\n","def cons_batch_graph(graphs):\n","    num_nodes = max([len(g['g_features']) for g in graphs])\n","    num_edges = max([g['num_edges'] for g in graphs])\n","\n","    batch_edges = []\n","    batch_node2edge = []\n","    batch_edge2node = []\n","    for g in graphs:\n","        edges = {}\n","        node2edge = lil_matrix(np.zeros((num_edges, num_nodes)), dtype=np.float32)\n","        edge2node = lil_matrix(np.zeros((num_nodes, num_edges)), dtype=np.float32)\n","        edge_index = 0\n","        for node1, value in g['g_adj'].items():\n","            node1 = int(node1)\n","            for each in value:\n","                node2 = int(each['node'])\n","                if node1 == node2: # Ignore self-loops for now\n","                    continue\n","                edges[edge_index] = each['edge']\n","                node2edge[edge_index, node2] = 1\n","                edge2node[node1, edge_index] = 1\n","                edge_index += 1\n","        batch_edges.append(edges)\n","        batch_node2edge.append(node2edge)\n","        batch_edge2node.append(edge2node)\n","    batch_graphs = {'max_num_edges': num_edges,\n","                    'edge_features': batch_edges,\n","                    'node2edge': batch_node2edge,\n","                    'edge2node': batch_edge2node\n","                    }\n","    return batch_graphs\n","\n","def vectorize_batch_graph(graph, edge_vocab=None, config=None):\n","    # # vectorize the graph\n","    # edge_features = []\n","    # for edges in graph['edge_features']:\n","    #     edges_v = []\n","    #     for idx in range(len(edges)):\n","    #         edges_v.append(edge_vocab.getIndex(edges[idx]))\n","    #     for _ in range(graph['max_num_edges'] - len(edges_v)):\n","    #         edges_v.append(edge_vocab.PAD)\n","    #     edge_features.append(edges_v)\n","\n","    # edge_features = torch.LongTensor(np.array(edge_features))\n","\n","    gv = {\n","          # 'edge_features': edge_features.to(config['device']) if config['device'] else edge_features,\n","          'node2edge': graph['node2edge'],\n","          'edge2node': graph['edge2node']\n","          }\n","    return gv\n","\n","class DummyLogger(object):\n","    def __init__(self, config, dirname=None, pretrained=None):\n","        self.config = config\n","        if dirname is None:\n","            if pretrained is None:\n","                raise Exception('Either --dir or --pretrained needs to be specified.')\n","            self.dirname = pretrained\n","        else:\n","            self.dirname = dirname\n","            if os.path.exists(dirname):\n","                raise Exception('Directory already exists: {}'.format(dirname))\n","            os.makedirs(dirname)\n","            os.mkdir(os.path.join(dirname, 'metrics'))\n","            self.log_json(config, os.path.join(self.dirname, _CONFIG_FILE))\n","        if config['logging']:\n","            self.f_metric = open(os.path.join(self.dirname, 'metrics', 'loss_f1_em.log'), 'a')\n","\n","    def log_json(self, data, filename, mode='w'):\n","        with open(filename, mode) as outfile:\n","            outfile.write(json.dumps(data, indent=4, ensure_ascii=False))\n","\n","    def log(self, data, filename):\n","        print(data)\n","\n","    def write_to_file(self, text):\n","        if self.config['logging']:\n","            self.f_metric.writelines(text + '\\n')\n","            self.f_metric.flush()\n","\n","    def close(self):\n","        if self.config['logging']:\n","            self.f_metric.close()\n","\n","class Logger(object):\n","    def __init__(self, log_file):\n","        self.terminal = sys.stdout\n","        self.log = open(log_file, \"a\")\n","\n","    def write(self, message):\n","        self.terminal.write(message)\n","        self.log.write(message)\n","        self.log.flush()\n","\n","    def flush(self):\n","        pass\n","\n","class Timer(object):\n","    \"\"\"Computes elapsed time.\"\"\"\n","    def __init__(self, name):\n","        self.name = name\n","        self.running = True\n","        self.total = 0\n","        self.start = round(time.time(), 2)\n","        self.intervalTime = round(time.time(), 2)\n","        print(\"<> <> <> Starting Timer [{}] <> <> <>\".format(self.name))\n","\n","    def reset(self):\n","        self.running = True\n","        self.total = 0\n","        self.start = round(time.time(), 2)\n","        return self\n","\n","    def interval(self, intervalName=''):\n","        intervalTime = self._to_hms(round(time.time() - self.intervalTime, 2))\n","        print(\"<> <> Timer [{}] <> <> Interval [{}]: {} <> <>\".format(self.name, intervalName, intervalTime))\n","        self.intervalTime = round(time.time(), 2)\n","        return intervalTime\n","\n","    def stop(self):\n","        if self.running:\n","            self.running = False\n","            self.total += round(time.time() - self.start, 2)\n","        return self\n","\n","    def resume(self):\n","        if not self.running:\n","            self.running = True\n","            self.start = round(time.time(), 2)\n","        return self\n","\n","    def time(self):\n","        if self.running:\n","            return round(self.total + time.time() - self.start, 2)\n","        return self.total\n","\n","    def finish(self):\n","        if self.running:\n","            self.running = False\n","            self.total += round(time.time() - self.start, 2)\n","            elapsed = self._to_hms(self.total)\n","        print(\"<> <> <> Finished Timer [{}] <> <> <> Total time elapsed: {} <> <> <>\".format(self.name, elapsed))\n","\n","    def _to_hms(self, seconds):\n","        m, s = divmod(seconds, 60)\n","        h, m = divmod(m, 60)\n","        return \"%dh %02dm %02ds\" % (h, m, s)\n","\n","\n","def dump_ndarray(data, path_to_file):\n","    try:\n","        with open(path_to_file, 'wb') as f:\n","            np.save(f, data)\n","    except Exception as e:\n","        raise e\n","\n","def load_ndarray(path_to_file):\n","    try:\n","        with open(path_to_file, 'rb') as f:\n","            data = np.load(f)\n","    except Exception as e:\n","        raise e\n","\n","    return data\n","\n","def dump_ndjson(data, file):\n","    try:\n","        with open(file, 'w') as f:\n","            for each in data:\n","                f.write(json.dumps(each) + '\\n')\n","    except Exception as e:\n","        raise e\n","\n","def load_ndjson(file, return_type='array'):\n","    if return_type == 'array':\n","        return load_ndjson_to_array(file)\n","    elif return_type == 'dict':\n","        return load_ndjson_to_dict(file)\n","    else:\n","        raise RuntimeError('Unknown return_type: %s' % return_type)\n","\n","def dump_json(data, file, indent=None):\n","    try:\n","        with open(file, 'w') as f:\n","            json.dump(data, f, indent=indent)\n","    except Exception as e:\n","        raise e\n","\n","def load_json(file):\n","    try:\n","        with open(file, 'r') as f:\n","            data = json.load(f)\n","    except Exception as e:\n","        raise e\n","    return data"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YCwBHzCRyxsy"},"source":["## Eval utils"]},{"cell_type":"code","metadata":{"id":"JM_O8JIvyzzw","executionInfo":{"status":"ok","timestamp":1610352653678,"user_tz":-420,"elapsed":89149,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["import numpy as np\n","import re\n","import string\n","from collections import Counter\n","\n","\n","################################################################################\n","# Text Processing Helper Functions #\n","################################################################################\n","\n","\n","def normalize_text(s):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    def remove_articles(text):\n","        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","\n","def compute_eval_metric(eval_metric, predictions, ground_truths, cross_eval=True):\n","    fns = {'f1': compute_f1_score,\n","           'em': compute_em_score}\n","\n","    def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n","        scores_for_ground_truths = []\n","        for ground_truth in ground_truths:\n","            score = metric_fn(normalize_text(prediction), normalize_text(ground_truth))\n","            scores_for_ground_truths.append(score)\n","        return max(scores_for_ground_truths)\n","\n","    values = []\n","    for prediction, ground_truth_set in zip(predictions, ground_truths):\n","        if cross_eval and len(ground_truth_set) > 1:\n","            _scores = []\n","            for i in range(len(ground_truth_set)):\n","                _ground_truth_set = []\n","                for j in range(len(ground_truth_set)):\n","                    if j != i:\n","                        _ground_truth_set.append(ground_truth_set[j])\n","                _scores.append(metric_max_over_ground_truths(fns[eval_metric], prediction, _ground_truth_set))\n","            value = np.mean(_scores)\n","        else:\n","            value = metric_max_over_ground_truths(fns[eval_metric], prediction, ground_truth_set)\n","        values.append(value)\n","    return np.mean(values)\n","\n","\n","def compute_f1_score(prediction, ground_truth):\n","    common = Counter(prediction.split()) & Counter(ground_truth.split())\n","    num_same = sum(common.values())\n","    if num_same == 0:\n","        return 0\n","    precision = 1.0 * num_same / len(prediction.split())\n","    recall = 1.0 * num_same / len(ground_truth.split())\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","\n","def compute_em_score(prediction, ground_truth):\n","    return 1.0 if prediction == ground_truth else 0.0"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RTjZBfBfznRW"},"source":["## Generic utils"]},{"cell_type":"code","metadata":{"id":"gxEPl5EWzpAf","executionInfo":{"status":"ok","timestamp":1610352653679,"user_tz":-420,"elapsed":89148,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["# import yaml\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","def get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None, device=None):\n","    ''' Sinusoid position encoding table '''\n","\n","    def cal_angle(position, hid_idx):\n","        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n","\n","    def get_posi_angle_vec(position):\n","        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n","\n","    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n","\n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n","\n","    if padding_idx is not None:\n","        # zero vector for padding dimension\n","        sinusoid_table[padding_idx] = 0.\n","\n","    sinusoid_table = torch.Tensor(sinusoid_table)\n","    return sinusoid_table.to(device) if device else sinusoid_table\n","\n","def get_range_vector(size, device):\n","    \"\"\"\n","    Returns a range vector with the desired size, starting at 0. The CUDA implementation\n","    is meant to avoid copy data from CPU to GPU.\n","    \"\"\"\n","    if device.type == 'cuda':\n","        return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n","    else:\n","        return torch.arange(0, size, dtype=torch.long)\n","\n","def to_cuda(x, device=None):\n","    if device:\n","        x = x.to(device)\n","    return x\n","\n","def batched_diag(x, device=None):\n","    # Input: a 2D tensor\n","    # Output: a 3D tensor\n","    x_diag = torch.zeros(x.size(0), x.size(1), x.size(1))\n","    _ = x_diag.as_strided(x.size(), [x_diag.stride(0), x_diag.size(2) + 1]).copy_(x)\n","    return to_cuda(x_diag, device)\n","\n","def create_mask(x, N, device=None):\n","    x = x.data\n","    mask = np.zeros((x.size(0), N))\n","    for i in range(x.size(0)):\n","        mask[i, :x[i]] = 1\n","    return to_cuda(torch.Tensor(mask), device)\n","\n","def get_config(config_path=\"config.yml\"):\n","    return config"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IdQQQounz16e"},"source":["## Layers"]},{"cell_type":"code","metadata":{"id":"pCF8n1TVz3Hd","executionInfo":{"status":"ok","timestamp":1610352655247,"user_tz":-420,"elapsed":90715,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["import torch\n","import torch.nn as nn\n","\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","import torch.nn.functional as F\n","\n","INF = 1e20\n","VERY_SMALL_NUMBER = 1e-12\n","COMBINE_RATIO = 0.9\n","\n","class Context2QuestionAttention(nn.Module):\n","    def __init__(self, dim, hidden_size):\n","        super(Context2QuestionAttention, self).__init__()\n","        self.linear_sim = nn.Linear(dim, hidden_size, bias=False)\n","\n","    def forward(self, context, questions, out_questions, ques_mask=None):\n","        \"\"\"\n","        Parameters\n","        :context, (batch_size, ?, ctx_size, dim)\n","        :questions, (batch_size, turn_size, ques_size, dim)\n","        :out_questions, (batch_size, turn_size, ques_size, ?)\n","        :ques_mask, (batch_size, turn_size, ques_size)\n","        Returns\n","        :ques_emb, (batch_size, turn_size, ctx_size, dim)\n","        \"\"\"\n","        # shape: (batch_size, ?, ctx_size, dim), ? equals 1 or turn_size\n","        context_fc = torch.relu(self.linear_sim(context))\n","        # shape: (batch_size, turn_size, ques_size, dim)\n","        questions_fc = torch.relu(self.linear_sim(questions))\n","\n","        # shape: (batch_size, turn_size, ctx_size, ques_size)\n","        attention = torch.matmul(context_fc, questions_fc.transpose(-1, -2))\n","        if ques_mask is not None:\n","            attention = attention.masked_fill_(1 - ques_mask.byte().unsqueeze(2), -INF)\n","        prob = torch.softmax(attention, dim=-1)\n","        # shape: (batch_size, turn_size, ctx_size, ?)\n","        ques_emb = torch.matmul(prob, out_questions)\n","        return ques_emb\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(SelfAttention, self).__init__()\n","        self.W1 = torch.Tensor(input_size, hidden_size)\n","        self.W1 = nn.Parameter(nn.init.xavier_uniform_(self.W1))\n","        self.W2 = torch.Tensor(hidden_size, 1)\n","        self.W2 = nn.Parameter(nn.init.xavier_uniform_(self.W2))\n","\n","    def forward(self, x, attention_mask=None):\n","        attention = torch.mm(torch.tanh(torch.mm(x.view(-1, x.size(-1)), self.W1)), self.W2).view(x.size(0), -1)\n","        if attention_mask is not None:\n","            # Exclude masked elements from the softmax\n","            attention = attention.masked_fill_(1 - attention_mask.byte(), -INF)\n","\n","        probs = torch.softmax(attention, dim=-1).unsqueeze(1)\n","        weighted_x = torch.bmm(probs, x).squeeze(1)\n","        return weighted_x\n","\n","class GatedFusion(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(GatedFusion, self).__init__()\n","        '''GatedFusion module'''\n","        self.fc_z = nn.Linear(4 * hidden_size, hidden_size, bias=True)\n","\n","    def forward(self, h_state, input):\n","        z = torch.sigmoid(self.fc_z(torch.cat([h_state, input, h_state * input, h_state - input], -1)))\n","        h_state = (1 - z) * h_state + z * input\n","        return h_state\n","\n","class GRUStep(nn.Module):\n","    def __init__(self, hidden_size, input_size):\n","        super(GRUStep, self).__init__()\n","        '''GRU module'''\n","        self.linear_z = nn.Linear(hidden_size + input_size, hidden_size, bias=False)\n","        self.linear_r = nn.Linear(hidden_size + input_size, hidden_size, bias=False)\n","        self.linear_t = nn.Linear(hidden_size + input_size, hidden_size, bias=False)\n","\n","    def forward(self, h_state, input):\n","        z = torch.sigmoid(self.linear_z(torch.cat([h_state, input], -1)))\n","        r = torch.sigmoid(self.linear_r(torch.cat([h_state, input], -1)))\n","        t = torch.tanh(self.linear_t(torch.cat([r * h_state, input], -1)))\n","        h_state = (1 - z) * h_state + z * t\n","        return h_state\n","\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, \\\n","        bidirectional=False, rnn_type='lstm', rnn_dropout=None, rnn_input_dropout=None, device=None):\n","        super(EncoderRNN, self).__init__()\n","        if not rnn_type in ('lstm', 'gru'):\n","            raise RuntimeError('rnn_type is expected to be lstm or gru, got {}'.format(rnn_type))\n","        if bidirectional:\n","            print('[ Using bidirectional {} encoder ]'.format(rnn_type))\n","        else:\n","            print('[ Using {} encoder ]'.format(rnn_type))\n","        if bidirectional and hidden_size % 2 != 0:\n","            raise RuntimeError('hidden_size is expected to be even in the bidirectional mode!')\n","        self.rnn_type = rnn_type\n","        self.rnn_dropout = rnn_dropout\n","        self.rnn_input_dropout = rnn_input_dropout\n","        self.device = device\n","        self.hidden_size = hidden_size // 2 if bidirectional else hidden_size\n","        self.num_directions = 2 if bidirectional else 1\n","        model = nn.LSTM if rnn_type == 'lstm' else nn.GRU\n","        self.model = model(input_size, self.hidden_size, 1, batch_first=True, bidirectional=bidirectional)\n","\n","    def forward(self, x, x_len):\n","        \"\"\"x: [batch_size * max_length * emb_dim]\n","           x_len: [batch_size]\n","        \"\"\"\n","        x = dropout(x, self.rnn_input_dropout, shared_axes=[-2], training=self.training)\n","        sorted_x_len, indx = torch.sort(x_len, 0, descending=True)\n","        x = pack_padded_sequence(x[indx], sorted_x_len.data.tolist(), batch_first=True)\n","\n","        h0 = to_cuda(torch.zeros(self.num_directions, x_len.size(0), self.hidden_size), self.device)\n","        if self.rnn_type == 'lstm':\n","            c0 = to_cuda(torch.zeros(self.num_directions, x_len.size(0), self.hidden_size), self.device)\n","            packed_h, (packed_h_t, _) = self.model(x, (h0, c0))\n","            packed_h_t = torch.cat([packed_h_t[i] for i in range(packed_h_t.size(0))], -1)\n","        else:\n","            packed_h, packed_h_t = self.model(x, h0)\n","            packed_h_t = packed_h_t.transpose(0, 1).contiguous().view(x_len.size(0), -1)\n","\n","        hh, _ = pad_packed_sequence(packed_h, batch_first=True)\n","\n","        # restore the sorting\n","        _, inverse_indx = torch.sort(indx, 0)\n","        restore_hh = hh[inverse_indx]\n","        restore_packed_h_t = packed_h_t[inverse_indx]\n","        restore_hh = dropout(restore_hh, self.rnn_dropout, shared_axes=[-2], training=self.training)\n","        restore_packed_h_t = dropout(restore_packed_h_t, self.rnn_dropout, training=self.training)\n","        return restore_hh, restore_packed_h_t\n","\n","\n","def dropout(x, drop_prob, shared_axes=[], training=False):\n","    \"\"\"\n","    Apply dropout to input tensor.\n","    Parameters\n","    ----------\n","    input_tensor: ``torch.FloatTensor``\n","        A tensor of shape ``(batch_size, ..., num_timesteps, embedding_dim)``\n","    Returns\n","    -------\n","    output: ``torch.FloatTensor``\n","        A tensor of shape ``(batch_size, ..., num_timesteps, embedding_dim)`` with dropout applied.\n","    \"\"\"\n","    if drop_prob == 0 or drop_prob == None or (not training):\n","        return x\n","\n","    sz = list(x.size())\n","    for i in shared_axes:\n","        sz[i] = 1\n","    mask = x.new(*sz).bernoulli_(1. - drop_prob).div_(1. - drop_prob)\n","    mask = mask.expand_as(x)\n","    return x * mask\n","\n","class GraphLearner(nn.Module):\n","    def __init__(self, input_size, hidden_size, topk, epsilon, n_spatial_kernels, use_spatial_kernels=True, \\\n","            use_position_enc=False, position_emb_size=10, max_position_distance=160, num_pers=1, device=None):\n","        super(GraphLearner, self).__init__()\n","        self.device = device\n","        self.topk = topk\n","        self.epsilon = epsilon\n","        self.use_spatial_kernels = use_spatial_kernels\n","        self.use_position_enc = use_position_enc\n","        self.max_position_distance = max_position_distance\n","        # self.linear_sim = nn.Linear(input_size, hidden_size, bias=False)\n","\n","        self.weight_tensor = torch.Tensor(num_pers, input_size)\n","        self.weight_tensor = nn.Parameter(nn.init.xavier_uniform_(self.weight_tensor))\n","        print('[ Multi-perspective GraphLearner: {} ]'.format(num_pers))\n","\n","\n","        if use_spatial_kernels:\n","            print('[ Using spatial Gaussian kernels ]')\n","            if use_position_enc:\n","                print('[ Using sinusoid position encoding ]')\n","                # Position encoding\n","                self.position_enc = nn.Embedding.from_pretrained(\n","                    get_sinusoid_encoding_table(self.max_position_distance + 1, position_emb_size, padding_idx=0, device=device),\n","                    freeze=True)\n","\n","                # Parameters of the Gaussian kernels\n","                self.mean_dis = nn.Parameter(torch.Tensor(n_spatial_kernels, position_emb_size))\n","                self.mean_dis.data.uniform_(-1, 1)\n","                self.precision_inv_dis = nn.Parameter(torch.Tensor(n_spatial_kernels, position_emb_size))\n","                self.precision_inv_dis.data.uniform_(0.0, 1.0)\n","            else:\n","                # Parameters of the Gaussian kernels\n","                self.mean_dis = nn.Parameter(torch.Tensor(n_spatial_kernels, 1))\n","                self.mean_dis.data.uniform_(0, 1)\n","                self.precision_inv_dis = nn.Parameter(torch.Tensor(n_spatial_kernels, 1))\n","                self.precision_inv_dis.data.uniform_(0.0, 1.0)\n","\n","    def forward(self, context, ctx_mask):\n","        \"\"\"\n","        Parameters\n","        :context, (batch_size, turn_size, ctx_size, dim)\n","        :ctx_mask, (batch_size, ctx_size)\n","        Returns\n","        :adjacency_matrix, (batch_size, turn_size, ctx_size, ctx_size)\n","        \"\"\"\n","        markoff_value = -INF\n","\n","        # 1)\n","        # context_fc = torch.relu(self.linear_sim(context))\n","        # attention = torch.matmul(context_fc, context_fc.transpose(-1, -2))\n","\n","        # # 2)\n","        # context_fc = context.unsqueeze(2) * self.weight_tensor.unsqueeze(0).unsqueeze(0).unsqueeze(-2)\n","        # attention = torch.mean(torch.matmul(context_fc, context_fc.transpose(-1, -2)), dim=2)\n","\n","\n","        # 3) Best attention mechanism\n","        context_fc = context.unsqueeze(2) * torch.relu(self.weight_tensor).unsqueeze(0).unsqueeze(0).unsqueeze(-2)\n","        attention = torch.mean(torch.matmul(context_fc, context.unsqueeze(2).transpose(-1, -2)), dim=2)\n","\n","\n","        # # 4）weighted cosine\n","        # context_fc = context.unsqueeze(2) * self.weight_tensor.unsqueeze(0).unsqueeze(0).unsqueeze(-2)\n","        # context_norm = F.normalize(context_fc, p=2, dim=-1)\n","        # attention = torch.matmul(context_norm, context_norm.transpose(-1, -2)).mean(2)\n","        # markoff_value = 0\n","\n","\n","        if ctx_mask is not None:\n","            attention = attention.masked_fill_(1 - ctx_mask.byte().unsqueeze(1).unsqueeze(-1), markoff_value)\n","            attention = attention.masked_fill_(1 - ctx_mask.byte().unsqueeze(1).unsqueeze(-2), markoff_value)\n","\n","        if self.use_spatial_kernels:\n","            # shape: (batch_size, turn_size, n_spatial_kernels, ctx_size, ctx_size)\n","            spatial_attention = self.get_spatial_attention(attention.shape[:3])\n","            # joint_attention = COMBINE_RATIO * torch.softmax(attention, dim=-1).unsqueeze(2) + (1 - COMBINE_RATIO) * spatial_attention / torch.sum(spatial_attention, dim=-1, keepdim=True)\n","            weighted_adjacency_matrix = self.build_knn_neighbourhood(attention, self.topk, attention, spatial_attention)\n","        else:\n","            if self.topk is not None:\n","                weighted_adjacency_matrix = self.build_knn_neighbourhood(attention, self.topk)\n","\n","            if self.epsilon is not None:\n","                weighted_adjacency_matrix = self.build_epsilon_neighbourhood(attention, self.epsilon, markoff_value)\n","\n","        return weighted_adjacency_matrix\n","\n","\n","    def build_epsilon_neighbourhood(self, attention, epsilon, markoff_value):\n","        mask = (attention > epsilon).detach().float()\n","        weighted_adjacency_matrix = attention * mask + markoff_value * (1 - mask)\n","        return weighted_adjacency_matrix\n","\n","\n","    def build_knn_neighbourhood(self, attention, topk, semantic_attention=None, spatial_attention=None, markoff_value=-INF):\n","        knn_val, knn_ind = torch.topk(attention, topk, dim=-1)\n","        if self.use_spatial_kernels:\n","            # semantic_attention = semantic_attention.unsqueeze(2).expand(-1, -1, spatial_attention.size(2), -1, -1)\n","            semantic_attn_chosen = torch.gather(semantic_attention, dim=-1, index=knn_ind)\n","            semantic_attn_chosen = torch.softmax(semantic_attn_chosen, dim=-1)\n","\n","            expand_knn_ind = knn_ind.unsqueeze(2).expand(-1, -1, spatial_attention.size(2), -1, -1)\n","            spatial_attn_chosen = torch.gather(spatial_attention, dim=-1, index=expand_knn_ind)\n","            spatial_attn_chosen = spatial_attn_chosen / torch.sum(spatial_attn_chosen, dim=-1, keepdim=True)\n","\n","            attn_chosen = semantic_attn_chosen.unsqueeze(2) * spatial_attn_chosen\n","            weighted_adjacency_matrix = to_cuda(torch.zeros_like(spatial_attention).scatter_(-1, expand_knn_ind, attn_chosen), self.device)\n","        else:\n","            weighted_adjacency_matrix = to_cuda((markoff_value * torch.ones_like(attention)).scatter_(-1, knn_ind, knn_val), self.device)\n","        return weighted_adjacency_matrix\n","\n","    def get_spatial_attention(self, shape):\n","        # Compute pseudo-coordinates for context words\n","        batch_size, turn_size, ctx_size = shape\n","        ctx_token_idx = get_range_vector(ctx_size, self.device)\n","        pseudo_coord = ctx_token_idx.unsqueeze(-1) - ctx_token_idx.unsqueeze(0)\n","        if self.use_position_enc:\n","            # Truncate\n","            pseudo_coord = torch.clamp(torch.abs(pseudo_coord) + 1, max=self.max_position_distance)\n","            pseudo_coord = self.position_enc(pseudo_coord)\n","            # Use Gaussian kernel to model attention over distance\n","            spatial_attention = self.get_multivariate_gaussian_weights(pseudo_coord)\n","        else:\n","            # Truncate & scale\n","            # pseudo_coord = torch.clamp(pseudo_coord, min=-self.max_position_distance, max=self.max_position_distance)\n","            pseudo_coord = torch.clamp(torch.abs(pseudo_coord.float()), max=self.max_position_distance) / self.max_position_distance\n","            # Use Gaussian kernel to model attention over distance\n","            spatial_attention = self.get_gaussian_weights(pseudo_coord)\n","\n","        # shape: (batch_size, turn_size, n_spatial_kernels, ctx_size, ctx_size)\n","        spatial_attention = spatial_attention.unsqueeze(0).unsqueeze(0).expand(batch_size, turn_size, -1, -1, -1)\n","        return spatial_attention\n","\n","    def get_gaussian_weights(self, pseudo_coord):\n","        '''\n","        ## Inputs:\n","        - pseudo_coord (ctx_size, ctx_size)\n","        ## Returns:\n","        - weights (n_spatial_kernels, ctx_size, ctx_size)\n","        '''\n","        # compute weights\n","        diff = (pseudo_coord.view(1, -1) - self.mean_dis)**2\n","        weights = torch.exp(-0.5 * diff * (self.precision_inv_dis**2))\n","\n","        # shape: (n_spatial_kernels, ctx_size, ctx_size)\n","        weights = weights.view((-1,) + pseudo_coord.shape)\n","        return weights\n","\n","    def get_multivariate_gaussian_weights(self, pseudo_coord):\n","        '''\n","        ## Inputs:\n","        - pseudo_coord (ctx_size, ctx_size, dim)\n","        ## Returns:\n","        - weights (n_spatial_kernels, ctx_size, ctx_size)\n","        '''\n","        # compute weights\n","        diff = (pseudo_coord.view(1, -1, pseudo_coord.size(-1)) - self.mean_dis.view(-1, 1, self.mean_dis.size(-1)))**2\n","        weights = torch.exp(-0.5 * torch.sum(diff * (self.precision_inv_dis.unsqueeze(1))**2, dim=-1))\n","\n","        # shape: (n_spatial_kernels, ctx_size, ctx_size)\n","        weights = weights.view((-1,) + pseudo_coord.shape[:2])\n","        return weights\n","\n","class ContextGraphNN(nn.Module):\n","    def __init__(self, hidden_size, n_spatial_kernels, use_spatial_kernels=True, graph_hops=1, bignn=False, device=None):\n","        super(ContextGraphNN, self).__init__()\n","        print('[ Using {}-hop ContextGraphNN ]'.format(graph_hops))\n","        self.graph_hops = graph_hops\n","        self.use_spatial_kernels = use_spatial_kernels\n","        if self.use_spatial_kernels:\n","            self.linear_kernels = nn.ModuleList([nn.Linear(hidden_size, hidden_size // n_spatial_kernels, bias=False) for _ in range(n_spatial_kernels)])\n","        else:\n","            n_spatial_kernels = 1\n","        self.gru_step = GRUStep(hidden_size, hidden_size // n_spatial_kernels * n_spatial_kernels)\n","        if bignn:\n","            self.gated_fusion = GatedFusion(hidden_size)\n","            self.update = self.bignn_update\n","        else:\n","            self.update = self.gnn_update\n","\n","        print('[ Using graph type: dynamic ]')\n","\n","\n","    def forward(self, node_state, weighted_adjacency_matrix):\n","        node_state = self.update(node_state, weighted_adjacency_matrix)\n","        return node_state\n","\n","    def bignn_update(self, node_state, weighted_adjacency_matrix):\n","        weighted_adjacency_matrix_in = torch.softmax(weighted_adjacency_matrix, dim=-1)\n","        weighted_adjacency_matrix_out = torch.softmax(weighted_adjacency_matrix.transpose(-1, -2), dim=-1)\n","\n","        for _ in range(self.graph_hops):\n","            agg_state_in = self.aggregate_avgpool(node_state, weighted_adjacency_matrix_in)\n","            agg_state_out = self.aggregate_avgpool(node_state, weighted_adjacency_matrix_out)\n","            agg_state = self.gated_fusion(agg_state_in, agg_state_out)\n","            node_state = self.gru_step(node_state, agg_state)\n","        return node_state\n","\n","    def gnn_update(self, node_state, weighted_adjacency_matrix):\n","        weighted_adjacency_matrix = torch.softmax(weighted_adjacency_matrix, dim=-1)\n","\n","\n","        for _ in range(self.graph_hops):\n","            agg_state = self.aggregate_avgpool(node_state, weighted_adjacency_matrix)\n","            node_state = self.gru_step(node_state, agg_state)\n","        return node_state\n","\n","    def aggregate_avgpool(self, node_state, weighted_adjacency_matrix):\n","        # Information aggregation\n","        if self.use_spatial_kernels:\n","            # Joint aggregation\n","            agg_state = torch.cat([self.linear_kernels[i](torch.matmul(weighted_adjacency_matrix[:, i], node_state)) for i in range(weighted_adjacency_matrix.size(1))], -1)\n","        else:\n","            agg_state = torch.matmul(weighted_adjacency_matrix, node_state)\n","        return agg_state\n","\n","\n","# Static GNN\n","class StaticContextGraphNN(nn.Module):\n","    def __init__(self, hidden_size, graph_hops=1, device=None):\n","        super(StaticContextGraphNN, self).__init__()\n","        print('[ Using {}-hop GraphNN ]'.format(graph_hops))\n","        self.device = device\n","        self.graph_hops = graph_hops\n","        self.linear_max = nn.Linear(hidden_size, hidden_size, bias=False)\n","\n","        # Static graph\n","        self.static_graph_mp = GraphMessagePassing()\n","        self.static_gated_fusion = GatedFusion(hidden_size)\n","        self.static_gru_step = GRUStep(hidden_size, hidden_size)\n","\n","        print('[ Using graph type: static ]')\n","\n","    def forward(self, node_state, adj):\n","        '''Static graph update'''\n","        node2edge, edge2node = adj\n","\n","        # Shape: (batch_size, num_edges, num_nodes)\n","        node2edge = to_cuda(torch.stack([torch.Tensor(x.A) for x in node2edge], dim=0), self.device)\n","        # Shape: (batch_size, num_nodes, num_edges)\n","        edge2node = to_cuda(torch.stack([torch.Tensor(x.A) for x in edge2node], dim=0), self.device)\n","\n","        for _ in range(self.graph_hops):\n","            bw_agg_state = self.static_graph_mp(node_state, node2edge, edge2node)\n","            fw_agg_state = self.static_graph_mp(node_state, edge2node.transpose(1, 2), node2edge.transpose(1, 2))\n","            agg_state = self.static_gated_fusion(fw_agg_state, bw_agg_state)\n","            node_state = self.static_gru_step(node_state, agg_state)\n","        return node_state\n","\n","\n","class GraphMessagePassing(nn.Module):\n","    def __init__(self):\n","        super(GraphMessagePassing, self).__init__()\n","\n","    def forward(self, node_state, node2edge, edge2node):\n","        node2edge_emb = torch.bmm(node2edge, node_state) # batch_size x num_edges x hidden_size\n","\n","        # Add self-loop\n","        norm_ = torch.sum(edge2node, 2, keepdim=True) + 1\n","        agg_state = (torch.bmm(edge2node, node2edge_emb) + node_state) / norm_\n","        return agg_state"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KdL57a1Dy14P"},"source":["## Word model"]},{"cell_type":"code","metadata":{"id":"55x-lr_Jy3du","executionInfo":{"status":"ok","timestamp":1610352655651,"user_tz":-420,"elapsed":91117,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["import os\n","import string\n","from collections import Counter\n","import numpy as np\n","from gensim.models.keyedvectors import KeyedVectors\n","\n","# from .utils import dump_ndarray, load_ndarray, dump_json, load_json, Timer\n","\n","\n","################################################################################\n","# WordModel Class #\n","################################################################################\n","\n","class GloveModel(object):\n","\n","    def __init__(self, filename):\n","        self.word_vecs = {}\n","        self.vocab = []\n","        with open(filename, 'r') as input_file:\n","            for line in input_file.readlines():\n","                splitLine = line.split(' ')\n","                w = splitLine[0]\n","                self.word_vecs[w] = np.array([float(val) for val in splitLine[1:]])\n","                self.vocab.append(w)\n","        self.vector_size = len(self.word_vecs[w])\n","\n","    def word_vec(self, word):\n","        word_list = [word, word.lower(), word.upper(), word.title(), string.capwords(word, '_')]\n","\n","        for w in word_list:\n","            if w in self.word_vecs:\n","                return self.word_vecs[w]\n","        return None\n","\n","\n","# class WordModel(object):\n","#     \"\"\"Class to get pretrained word vectors for a list of sentences. Can be used\n","#     for any pretrained word vectors.\n","#     \"\"\"\n","\n","#     def __init__(self, saved_vocab_file=None, embed_size=None, filename=None, embed_type='glove', top_n=None, additional_vocab=Counter()):\n","#         vocab_path = saved_vocab_file + '.vocab'\n","#         word_vec_path = saved_vocab_file + '.npy'\n","#         if os.path.exists(vocab_path) and \\\n","#                     os.path.exists(word_vec_path):\n","#             print('Loading pre-built vocabs stored in {}'.format(saved_vocab_file))\n","#             self.vocab = load_json(vocab_path)\n","#             self.word_vecs = load_ndarray(word_vec_path)\n","#             self.vocab_size = len(self.vocab) + 1\n","#             self.embed_size = self.word_vecs.shape[1]\n","#             assert self.embed_size == embed_size\n","#         else:\n","#             print('Building vocabs...')\n","#             if filename is None:\n","#                 if embed_size is None:\n","#                     raise Exception('Either embed_file or embed_size needs to be specified.')\n","#                 self.embed_size = embed_size\n","#                 self._model = None\n","#             else:\n","#                 self.set_model(filename, embed_type)\n","#                 self.embed_size = self._model.vector_size\n","\n","#             # padding: 0\n","#             self.vocab = {_UNK_TOKEN: 1, _QUESTION_SYMBOL: 2, _ANSWER_SYMBOL: 3}\n","#             n_added = 0\n","#             for w, count in additional_vocab.most_common():\n","#                 if w not in self.vocab:\n","#                     self.vocab[w] = len(self.vocab) + 1\n","#                     n_added += 1\n","#             # print('Added {} words to the vocab in total.'.format(n_added))\n","\n","#             self.vocab_size = len(self.vocab) + 1\n","#             print('Vocab size: {}'.format(self.vocab_size))\n","#             # self.word_vecs = np.random.rand(self.vocab_size, self.embed_size) * 0.2 - 0.1\n","#             self.word_vecs = np.random.uniform(-0.08, 0.08, (self.vocab_size, self.embed_size))\n","#             i = 0.\n","#             if self._model is not None:\n","#                 for word in self.vocab:\n","#                     emb = self._model.word_vec(word)\n","#                     if emb is not None:\n","#                         i += 1\n","#                         self.word_vecs[self.vocab[word]] = emb\n","#             self.word_vecs[0] = 0\n","#             print('Get_wordemb hit ratio: {}'.format(i / len(self.vocab)))\n","#             dump_json(self.vocab, vocab_path)\n","#             print('Saved vocab to {}'.format(vocab_path))\n","#             dump_ndarray(self.word_vecs, word_vec_path)\n","#             print('Saved word_vecs to {}'.format(word_vec_path))\n","\n","#     def set_model(self, filename, embed_type='glove'):\n","#         timer = Timer('Load {}'.format(filename))\n","#         if embed_type == 'glove':\n","#             self._model = GloveModel(filename)\n","#         else:\n","#             from gensim.models.keyedvectors import KeyedVectors\n","#             self._model = KeyedVectors.load_word2vec_format(filename, binary=True\n","#                                                             if embed_type == 'word2vec' else False)\n","#         print('Embeddings: vocab = {}, embed_size = {}'.format(len(self._model.vocab), self._model.vector_size))\n","#         timer.finish()\n","\n","#     def get_vocab(self):\n","#         return self.vocab\n","\n","#     def get_word_vecs(self):\n","#         return self.word_vecs\n","\n","class WordModel(object):\n","    \"\"\"Class to get pretrained word vectors for a list of sentences. Can be used\n","    for any pretrained word vectors.\n","    \"\"\"\n","\n","    def __init__(self, embed_size=None, filename=None, embed_type='glove', top_n=None, additional_vocab=Counter()):\n","        if filename is None:\n","            if embed_size is None:\n","                raise Exception('Either embed_file or embed_size needs to be specified.')\n","            self.embed_size = embed_size\n","            self._model = None\n","        else:\n","            self.set_model(filename, embed_type)\n","            self.embed_size = self._model.vector_size\n","\n","        # padding: 0\n","        self.vocab = {_UNK_TOKEN: 1, _QUESTION_SYMBOL: 2, _ANSWER_SYMBOL: 3}\n","        if self._model is not None:\n","            for i, key in enumerate(self._model.vocab):\n","                if (top_n is not None) and (i >= top_n):\n","                    break\n","                self.vocab[key] = len(self.vocab) + 1\n","\n","        n_added = 0\n","        for w, count in additional_vocab.most_common():\n","            if w not in self.vocab:\n","                self.vocab[w] = len(self.vocab) + 1\n","                n_added += 1\n","                if n_added <= 10:\n","                    print('Added word: {} (train_freq = {})'.format(w, count))\n","        print('Added {} words to the vocab in total.'.format(n_added))\n","\n","        self.vocab_size = len(self.vocab) + 1\n","        self.word_vecs = np.random.rand(self.vocab_size, self.embed_size) * 0.2 - 0.1\n","        for word in self.vocab:\n","            idx = self.vocab[word]\n","            if word in self._model.vocab:\n","                self.word_vecs[idx] = self._model.word_vec(word)\n","\n","    def set_model(self, filename, embed_type='glove'):\n","        timer = Timer('Load {}'.format(filename))\n","        if embed_type == 'glove':\n","            self._model = GloveModel(filename)\n","        else:\n","            self._model = KeyedVectors.load_word2vec_format(filename, binary=True\n","                                                            if embed_type == 'word2vec' else False)\n","        print('Embeddings: vocab = {}, embed_size = {}'.format(len(self._model.vocab), self._model.vector_size))\n","        timer.finish()\n","\n","    def get_vocab(self):\n","        return self.vocab\n","\n","    def get_word_vecs(self):\n","        return self.word_vecs"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QHrI4kW0zZlv"},"source":["## GraphFlow"]},{"cell_type":"code","metadata":{"id":"6vsGlORUzb6T","executionInfo":{"status":"ok","timestamp":1610352656797,"user_tz":-420,"elapsed":92263,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# from ..utils.generic_utils import *\n","# from ..layers.common import *\n","# from ..layers.attention import *\n","# from ..layers.graphs import *\n","\n","\n","INF = 1e20\n","class GraphFlow(nn.Module):\n","    def __init__(self, config, w_embedding):\n","        super(GraphFlow, self).__init__()\n","        self.config = config\n","        self.device = config['device']\n","        vocab_embed_size = config['vocab_embed_size']\n","        hidden_size = config['hidden_size']\n","\n","        # Extra features\n","        self.n_history = config['n_history']\n","        self.f_qem = config['f_qem']\n","        self.f_pos = config['f_pos']\n","        self.f_ner = config['f_ner']\n","        self.f_tf = config['f_tf']\n","        ctx_exact_match_embed_dim = config['ctx_exact_match_embed_dim']\n","        ctx_pos_embed_dim = config['ctx_pos_embed_dim']\n","        ctx_ner_embed_dim = config['ctx_ner_embed_dim']\n","        answer_marker_embed_dim = config['answer_marker_embed_dim']\n","        ques_marker_embed_dim = config['ques_marker_embed_dim']\n","        ques_turn_marker_embed_dim = config['ques_turn_marker_embed_dim']\n","\n","        self.use_bert = config['use_bert']\n","        self.finetune_bert = config.get('finetune_bert', None)\n","        self.use_bert_weight = config['use_bert_weight']\n","        bert_dim = config['bert_dim']\n","\n","        self.use_spatial_kernels = config['use_spatial_kernels']\n","        n_spatial_kernels = config['n_spatial_kernels'] if self.use_spatial_kernels else 1\n","\n","        self.word_dropout = config['word_dropout']\n","        self.bert_dropout = config['bert_dropout']\n","        self.rnn_dropout = config['rnn_dropout']\n","        self.rnn_input_dropout = config['rnn_input_dropout']\n","        self.ctx_graph_topk = config.get('ctx_graph_topk', None)\n","        self.ctx_graph_epsilon = config.get('ctx_graph_epsilon', None)\n","        self.static_graph = config.get('static_graph', None)\n","        self.word_embed = w_embedding\n","        if config['fix_vocab_embed']:\n","            print('[ Fix word embeddings ]')\n","            for param in self.word_embed.parameters():\n","                param.requires_grad = False\n","\n","        ctx_feature_dim = 0\n","        if self.f_qem:\n","            self.ctx_exact_match_embed = nn.Embedding(config['num_features_f_qem'], ctx_exact_match_embed_dim)\n","            ctx_feature_dim += ctx_exact_match_embed_dim\n","        if self.f_pos:\n","            self.ctx_pos_embed = nn.Embedding(config['num_features_f_pos'], ctx_pos_embed_dim)\n","            ctx_feature_dim += ctx_pos_embed_dim\n","        if self.f_ner:\n","            self.ctx_ner_embed = nn.Embedding(config['num_features_f_ner'], ctx_ner_embed_dim)\n","            ctx_feature_dim += ctx_ner_embed_dim\n","        if self.f_tf:\n","            ctx_feature_dim += 1\n","\n","        if self.n_history > 0:\n","            if answer_marker_embed_dim != 0:\n","                self.ctx_ans_marker_embed = nn.Embedding((self.n_history * 4) + 1, answer_marker_embed_dim)\n","            if config['use_ques_marker']:\n","                self.ques_marker_embed = nn.Embedding(sum([not config['no_pre_question'], not config['no_pre_answer']]) * self.n_history + 1, ques_marker_embed_dim)\n","                ques_turn_marker_embed_dim = 0\n","            else:\n","                self.ques_num_marker_embed = nn.Embedding(config['max_turn_num'], ques_turn_marker_embed_dim)\n","\n","        if self.use_bert and self.use_bert_weight:\n","            bert_layer_start, bert_layer_end = config['bert_layer_indexes'].split(',')\n","            num_bert_layers = int(bert_layer_end) - int(bert_layer_start)\n","            self.logits_bert_layers = nn.Parameter(nn.init.xavier_uniform_(torch.Tensor(1, num_bert_layers)))\n","            if config['use_bert_gamma']:\n","                self.gamma_bert_layers = nn.Parameter(nn.init.constant_(torch.Tensor(1, 1), 1.))\n","\n","\n","        self.ques_enc = EncoderRNN(vocab_embed_size + (ques_turn_marker_embed_dim if self.n_history > 0 else 0) + (ques_marker_embed_dim if self.n_history > 0 and config['use_ques_marker'] else 0) + (bert_dim if self.use_bert else 0), hidden_size, \\\n","                        bidirectional=True, \\\n","                        rnn_type='lstm', \\\n","                        rnn_dropout=self.rnn_dropout, \\\n","                        rnn_input_dropout=self.rnn_input_dropout, \\\n","                        device=self.device)\n","\n","        self.ctx_enc_l1 = EncoderRNN(2 * vocab_embed_size + ctx_feature_dim + self.n_history * answer_marker_embed_dim + (bert_dim if self.use_bert else 0), hidden_size, \\\n","                        bidirectional=True, \\\n","                        rnn_type='lstm', \\\n","                        rnn_dropout=self.rnn_dropout, \\\n","                        rnn_input_dropout=self.rnn_input_dropout, \\\n","                        device=self.device)\n","        self.ctx_enc_l2 = EncoderRNN(2 * hidden_size, hidden_size, \\\n","                        bidirectional=True, \\\n","                        rnn_type='lstm', \\\n","                        rnn_dropout=self.rnn_dropout, \\\n","                        rnn_input_dropout=self.rnn_input_dropout, \\\n","                        device=self.device)\n","        self.rnn_ques_over_time = EncoderRNN(1 * hidden_size, hidden_size, \\\n","                        bidirectional=False, \\\n","                        rnn_type='lstm', \\\n","                        rnn_dropout=self.rnn_dropout, \\\n","                        rnn_input_dropout=self.rnn_input_dropout, \\\n","                        device=self.device)\n","\n","        # Question attention\n","        self.ctx2ques_attn = Context2QuestionAttention(vocab_embed_size, hidden_size)\n","        self.ctx2ques_attn_l2 = Context2QuestionAttention(vocab_embed_size + hidden_size + (bert_dim if self.use_bert else 0), hidden_size)\n","        self.ques_self_atten = SelfAttention(hidden_size, hidden_size)\n","\n","        if config['use_gnn']:\n","            # Graph computation\n","            if self.static_graph:\n","                self.ctx_gnn = StaticContextGraphNN(hidden_size,  graph_hops=config['ctx_graph_hops'], device=self.device)\n","                self.ctx_gnn_l2 = StaticContextGraphNN(hidden_size, graph_hops=config['ctx_graph_hops'], device=self.device)\n","            else:\n","                self.graph_learner = GraphLearner(2 * vocab_embed_size + ctx_feature_dim + self.n_history * answer_marker_embed_dim + (bert_dim if self.use_bert else 0), \\\n","                            hidden_size, self.ctx_graph_topk, self.ctx_graph_epsilon, n_spatial_kernels, use_spatial_kernels=self.use_spatial_kernels, use_position_enc=config['use_position_enc'], \\\n","                            position_emb_size=config['position_emb_size'], max_position_distance=config['max_position_distance'], num_pers=config['graph_learner_num_pers'], device=self.device)\n","                self.ctx_gnn = ContextGraphNN(hidden_size, n_spatial_kernels, use_spatial_kernels=self.use_spatial_kernels, graph_hops=config['ctx_graph_hops'], bignn=config['bignn'], device=self.device)\n","                self.ctx_gnn_l2 = ContextGraphNN(hidden_size, n_spatial_kernels, use_spatial_kernels=self.use_spatial_kernels, graph_hops=config['ctx_graph_hops'], bignn=config['bignn'], device=self.device)\n","\n","            if config['temporal_gnn']:\n","                self.graph_gru_step = GatedFusion(hidden_size)\n","                self.graph_gru_step_l2 = GatedFusion(hidden_size)\n","\n","\n","        # Prediction\n","        self.gru_step = GRUStep(hidden_size, hidden_size)\n","        self.linear_start = nn.Linear(hidden_size, hidden_size, bias=False)\n","        self.linear_end = nn.Linear(hidden_size, hidden_size, bias=False)\n","        if self.config['dataset_name'].lower() == 'coqa':\n","            self.fc_clf = nn.Linear(hidden_size, 2 * hidden_size * self.config['coqa_answer_class_num'], bias=True)\n","        elif self.config['dataset_name'].lower() in ('quac', 'doqa'):\n","            self.linear_unk_answer = nn.Linear(hidden_size, 2 * hidden_size, bias=False)\n","            self.fc_followup = nn.Linear(hidden_size, 2 * hidden_size * self.config['quac_followup_class_num'], bias=True)\n","            self.fc_yesno = nn.Linear(hidden_size, 2 * hidden_size * self.config['quac_yesno_class_num'], bias=True)\n","        else:\n","            raise ValueError('Unknown dataset name: {}'.format(self.config['dataset_name']))\n","\n","\n","    def forward(self, ex):\n","        \"\"\"\n","        Parameters\n","        :questions, (batch_size, turn_size, ques_size)\n","        :bert_questions_f, (batch_size, turn_size, ques_size, bert_dim)\n","        :ques_len, (batch_size, turn_size)\n","        :context, (batch_size, ctx_size)\n","        :context_f, dict, val shape: (batch_size, turn_size, ctx_size)\n","        :bert_context_f, (batch_size, ctx_size, bert_dim)\n","        :context_ans_marker, (batch_size, turn_size, ctx_size, n_history)\n","        :ctx_len, (batch_size,)\n","        :num_turn, (batch_size,)\n","        Returns\n","        :start_logits, (batch_size, turn_size, ctx_size)\n","        :end_logits, (batch_size, turn_size, ctx_size)\n","        :unk_answer_logits, (batch_size, turn_size)\n","        :yes_answer_logits, (batch_size, turn_size)\n","        :no_answer_logits, (batch_size, turn_size)\n","        \"\"\"\n","        questions = ex['xq']\n","        ques_len = ex['xq_len']\n","        context = ex['xd']\n","        ctx_len = ex['xd_len']\n","        num_turn = ex['num_turn']\n","\n","        if self.use_bert:\n","            bert_questions_f = ex['bert_xq_f']\n","            bert_context_f = ex['bert_xd_f']\n","            if not self.finetune_bert:\n","                assert bert_questions_f.requires_grad == False\n","                assert bert_context_f.requires_grad == False\n","            if self.use_bert_weight:\n","                weights_bert_layers = torch.softmax(self.logits_bert_layers, dim=-1)\n","                if self.config['use_bert_gamma']:\n","                    weights_bert_layers = weights_bert_layers * self.gamma_bert_layers\n","\n","                bert_questions_f = torch.mm(weights_bert_layers, bert_questions_f.view(bert_questions_f.size(0), -1)).view(bert_questions_f.shape[1:])\n","                bert_questions_f = dropout(bert_questions_f, self.bert_dropout, shared_axes=[-2], training=self.training)\n","\n","                bert_context_f = torch.mm(weights_bert_layers, bert_context_f.view(bert_context_f.size(0), -1)).view(bert_context_f.shape[1:])\n","                bert_context_f = dropout(bert_context_f, self.bert_dropout, shared_axes=[-2], training=self.training)\n","\n","        ques_mask = create_mask(ques_len.view(-1), questions.size(-1), self.device)\n","        ctx_mask = create_mask(ctx_len, context.size(-1), self.device)\n","        expand_ctx_len = ctx_len.unsqueeze(1).expand(-1, questions.size(1)).contiguous()\n","        turn_mask = create_mask(num_turn, questions.size(1), self.device)\n","\n","        # Encoding module\n","        # Encode questions & context\n","        # shape: (batch_size * turn_size, ques_size, emb_dim)\n","        ques_emb = self.word_embed(questions.view(-1, questions.size(-1)))\n","        ques_emb = dropout(ques_emb, self.word_dropout, shared_axes=[-2], training=self.training)\n","        # shape: (batch_size, ctx_size, emb_dim)\n","        ctx_emb = self.word_embed(context)\n","        ctx_emb = dropout(ctx_emb, self.word_dropout, shared_axes=[-2], training=self.training)\n","\n","        # shape: (batch_size, turn_size, ctx_size, emb_dim)\n","        ctx_aware_ques_emb = self.ctx2ques_attn(ctx_emb.unsqueeze(1), ques_emb.view(questions.size() + (-1,)), \\\n","                                ques_emb.view(questions.size() + (-1,)), ques_mask.view(ques_len.size() + (-1,)))\n","\n","\n","        ques_cat_0 = [ques_emb]\n","        ctx_cat_0 = [ctx_emb.unsqueeze(1).expand(-1, questions.size(1), -1, -1), ctx_aware_ques_emb]\n","        # Add extra context features\n","        if self.f_qem:\n","            context_f_qem = self.ctx_exact_match_embed(ex['xd_f']['f_qem'].view(-1, ex['xd_f']['f_qem'].size(-1))).view(ex['xd_f']['f_qem'].shape[:3] + (-1,))\n","            ctx_cat_0.append(context_f_qem)\n","        if self.f_pos:\n","            context_f_pos = self.ctx_pos_embed(ex['xd_f']['f_pos'].view(-1, ex['xd_f']['f_pos'].size(-1))).view(ex['xd_f']['f_pos'].shape[:3] + (-1,))\n","            ctx_cat_0.append(context_f_pos)\n","        if self.f_ner:\n","            context_f_ner = self.ctx_ner_embed(ex['xd_f']['f_ner'].view(-1, ex['xd_f']['f_ner'].size(-1))).view(ex['xd_f']['f_ner'].shape[:3] + (-1,))\n","            ctx_cat_0.append(context_f_ner)\n","        if self.f_tf:\n","            context_f_tf = ex['xd_tf'].unsqueeze(1).unsqueeze(-1).expand(-1, questions.size(1), -1, -1)\n","            ctx_cat_0.append(context_f_tf)\n","\n","        if self.n_history > 0:\n","            # Encode previous N answer locations to the context embeddings\n","            # shape: (batch_size, turn_size, ctx_size, n_history * answer_marker_embed_dim)\n","            if self.config['answer_marker_embed_dim'] != 0:\n","                context_ans_marker = ex['xd_answer_marker']\n","                ctx_ans_marker_emb = self.ctx_ans_marker_embed(context_ans_marker.view(-1, context_ans_marker.size(-1))) \\\n","                                        .view(context_ans_marker.shape[:3] + (-1,))\n","                ctx_cat_0.append(ctx_ans_marker_emb)\n","\n","            if self.config['use_ques_marker']:\n","                question_f = ex['xq_f']\n","                question_f = self.ques_marker_embed(question_f.view(-1, question_f.size(-1)))\n","                ques_cat_0.append(question_f)\n","            else:\n","                # Encode question turn number inside the dialog into question embeddings\n","                question_num_ind = get_range_vector(questions.size(1), self.device)\n","                question_num_ind = question_num_ind.unsqueeze(-1).expand(-1, questions.size(-1))\n","                question_num_ind = question_num_ind.unsqueeze(0).expand(questions.size(0), -1, -1)\n","                question_num_ind = question_num_ind.reshape(-1, questions.size(-1))\n","                question_num_marker_emb = self.ques_num_marker_embed(question_num_ind)\n","                ques_cat_0.append(question_num_marker_emb)\n","\n","        if self.use_bert:\n","            ques_cat_0.append(bert_questions_f.view((-1,) + bert_questions_f.shape[-2:]))\n","            ctx_cat_0.append(bert_context_f.unsqueeze(1).expand(-1, questions.size(1), -1, -1))\n","\n","        ques_hidden_state_0 = torch.cat(ques_cat_0, dim=-1)\n","        ctx_hidden_state_0 = torch.cat(ctx_cat_0, dim=-1)\n","\n","        # Run RNN on questions\n","        # shape: (batch_size * turn_size, ques_size, hidden_size)\n","        ques_hidden_state = self.ques_enc(ques_hidden_state_0, ques_len.view(-1))[0]\n","\n","        # shape: (batch_size, turn_size, hidden_size)\n","        ques_state = self.ques_self_atten(ques_hidden_state, ques_mask).view(ques_len.size() + (-1,))\n","\n","\n","        # Reasoning module\n","\n","        # Run GNN on context graphs Layer 1\n","        # shape: (batch_size, turn_size, ctx_size, hidden_size)\n","        ctx_node_state_l1 = self.ctx_enc_l1(ctx_hidden_state_0.view(-1, ctx_hidden_state_0.size(-2), ctx_hidden_state_0.size(-1)), expand_ctx_len.view(-1))[0]\\\n","                    .view(ctx_hidden_state_0.shape[:3] + (-1,))\n","\n","\n","        if self.config['use_gnn']:\n","            if self.static_graph:\n","                input_graphs = ex['xd_graphs']\n","                ctx_adjacency_matrix = (input_graphs['node2edge'], input_graphs['edge2node'])\n","            else:\n","                # Construct context graphs\n","                # shape: (batch_size, turn_size, n_spatial_kernels, ctx_size, ctx_size)\n","                # or (batch_size, turn_size, ctx_size, ctx_size)\n","                ctx_adjacency_matrix = self.graph_learner(ctx_hidden_state_0, ctx_mask)\n","\n","\n","            for turn_id in range(questions.size(1)):\n","                # shape: (batch_size, ctx_size, hidden_size)\n","                if self.static_graph:\n","                    ctx_turn_node_state = self.ctx_gnn(ctx_node_state_l1[:, turn_id].clone(), ctx_adjacency_matrix)\n","                else:\n","                    ctx_turn_node_state = self.ctx_gnn(ctx_node_state_l1[:, turn_id].clone(), ctx_adjacency_matrix[:, turn_id])\n","                ctx_node_state_l1[:, turn_id] = ctx_turn_node_state\n","                if self.config['temporal_gnn']:\n","                    next_turn_id = turn_id + 1\n","                    if next_turn_id < questions.size(1):\n","                        ctx_node_state_l1[:, next_turn_id] = self.graph_gru_step(ctx_turn_node_state, ctx_node_state_l1[:, next_turn_id].clone())\n","\n","\n","\n","        if self.config.get('stacked_layer', True):\n","            # Run GNN on context graphs Layer 2\n","            ctx_cat_l2 = torch.cat([ctx_node_state_l1, ctx_emb.unsqueeze(1).expand(-1, questions.size(1), -1, -1)], -1)\n","            ques_cat_l2 = torch.cat([ques_hidden_state.view(questions.size() + (-1,)), ques_emb.view(questions.size() + (-1,))], -1)\n","            if self.use_bert:\n","                ctx_cat_l2 = torch.cat([ctx_cat_l2, bert_context_f.unsqueeze(1).expand(-1, questions.size(1), -1, -1)], -1)\n","                ques_cat_l2 = torch.cat([ques_cat_l2, bert_questions_f], -1)\n","            ctx_aware_ques_emb_l2 = self.ctx2ques_attn_l2(ctx_cat_l2, ques_cat_l2, ques_hidden_state.view(questions.size() + (-1,)), \\\n","                ques_mask.view(ques_len.size() + (-1,)))\n","            # Shape: (batch_size, turn_size, ctx_size, 2 * hidden_size)\n","            ctx_hidden_state_1 = torch.cat([ctx_node_state_l1, ctx_aware_ques_emb_l2], -1)\n","\n","\n","            ctx_node_state_l2 = self.ctx_enc_l2(ctx_hidden_state_1.view(-1, ctx_hidden_state_1.size(-2), ctx_hidden_state_1.size(-1)), expand_ctx_len.view(-1))[0]\\\n","                        .view(ctx_hidden_state_1.shape[:3] + (-1,))\n","\n","\n","            if self.config['use_gnn']:\n","                for turn_id in range(questions.size(1)):\n","                    # shape: (batch_size, ctx_size, hidden_size)\n","                    if self.static_graph:\n","                        ctx_turn_node_state = self.ctx_gnn_l2(ctx_node_state_l2[:, turn_id].clone(), ctx_adjacency_matrix)\n","                    else:\n","                        ctx_turn_node_state = self.ctx_gnn_l2(ctx_node_state_l2[:, turn_id].clone(), ctx_adjacency_matrix[:, turn_id])\n","                    ctx_node_state_l2[:, turn_id] = ctx_turn_node_state\n","                    if self.config['temporal_gnn']:\n","                        next_turn_id = turn_id + 1\n","                        if next_turn_id < questions.size(1):\n","                            ctx_node_state_l2[:, next_turn_id] = self.graph_gru_step_l2(ctx_turn_node_state, ctx_node_state_l2[:, next_turn_id].clone())\n","\n","            ctx_node_state_final = ctx_node_state_l2\n","\n","        else:\n","            ctx_node_state_final = ctx_node_state_l1\n","\n","\n","\n","        # Prediction module\n","        ques_state = self.rnn_ques_over_time(ques_state, num_turn.view(-1))[0]\n","\n","\n","        # Answer span prediction\n","        p_start = ques_state\n","\n","        # shape: (batch_size, turn_size, ctx_size)\n","        start_ = torch.matmul(ctx_node_state_final, self.linear_start(p_start).unsqueeze(-1)).squeeze(-1)\n","        start_ = start_.masked_fill_(1 - ctx_mask.byte().unsqueeze(1), -INF)\n","        start_logits = F.log_softmax(start_, dim=-1)\n","        start_probs = torch.exp(start_logits).unsqueeze(2)\n","\n","        p_end = self.gru_step(p_start, torch.matmul(start_probs, ctx_node_state_final).squeeze(2))\n","        end_ = torch.matmul(ctx_node_state_final, self.linear_end(p_end).unsqueeze(-1)).squeeze(-1)\n","        end_ = end_.masked_fill_(1 - ctx_mask.byte().unsqueeze(1), -INF)\n","        end_logits = F.log_softmax(end_, dim=-1)\n","        # end_probs = torch.exp(end_logits).unsqueeze(2)\n","\n","        # ctx_sum = torch.matmul(start_probs * end_probs, ctx_node_state_final).squeeze(2)\n","        # UNK/Yes/No prediction\n","        ctx_mean = torch.mean(ctx_node_state_final, dim=2)\n","        ctx_max = torch.max(ctx_node_state_final, dim=2)[0]\n","        # shape: (batch_size, turn_size, 2 * hidden_size, 1)\n","        ctx_cat = torch.cat([ctx_mean, ctx_max], -1).unsqueeze(-1)\n","\n","        # Answer type prediction\n","        if self.config['dataset_name'].lower() == 'coqa':\n","            p_clf = self.fc_clf(p_start).view(p_start.shape[:2] + (self.config['coqa_answer_class_num'], -1))\n","            score_c = torch.matmul(p_clf, ctx_cat).squeeze(-1)\n","            return {'start_logits': start_logits,\n","                    'end_logits': end_logits,\n","                    'score_c': score_c,\n","                    'turn_mask': turn_mask}\n","        else:\n","            unk_answer_probs = torch.sigmoid(torch.matmul(self.linear_unk_answer(p_start).unsqueeze(2), ctx_cat).squeeze(-1).squeeze(-1))\n","\n","            p_yesno = self.fc_yesno(p_start).view(p_start.shape[:2] + (self.config['quac_yesno_class_num'], -1))\n","            score_yesno = torch.matmul(p_yesno, ctx_cat).squeeze(-1)\n","\n","            p_followup = self.fc_followup(p_start).view(p_start.shape[:2] + (self.config['quac_followup_class_num'], -1))\n","            score_followup = torch.matmul(p_followup, ctx_cat).squeeze(-1)\n","            return {'start_logits': start_logits,\n","                    'end_logits': end_logits,\n","                    'unk_probs': unk_answer_probs,\n","                    'score_yesno': score_yesno,\n","                    'score_followup': score_followup,\n","                    'turn_mask': turn_mask}"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"il_GNyJzwJsH"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"HZGzH-F0wK3i","executionInfo":{"status":"ok","timestamp":1610352658249,"user_tz":-420,"elapsed":93713,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["import os\n","import numpy as np\n","from collections import Counter\n","import abc\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import torch.nn.functional as F\n","\n","\n","# from .utils.coqa import compute_eval_metric\n","# from .utils.quac import eval_fn as quac_eval_fn\n","# from .utils.doqa import eval_fn as doqa_eval_fn\n","# from .utils import constants as Constants\n","# from .word_model import WordModel\n","# from .models.graphflow import GraphFlow\n","# from .utils.radam import RAdam\n","\n","\n","class Model(metaclass=abc.ABCMeta):\n","    \"\"\"High level model that handles intializing the underlying network\n","    architecture, saving, updating examples, and predicting examples.\n","    \"\"\"\n","\n","    def __init__(self, config, train_set=None):\n","        # Book-keeping.\n","        self.config = config\n","        if self.config['pretrained']:\n","            state_dict_opt = self.init_saved_network(self.config['pretrained'])\n","        else:\n","            assert train_set is not None\n","            print('Train vocab: {}'.format(len(train_set.vocab)))\n","            vocab = Counter()\n","            for w in train_set.vocab:\n","                if train_set.vocab[w] >= config['min_freq']:\n","                    vocab[w] = train_set.vocab[w]\n","            print('Pruned train vocab: {}'.format(len(vocab)))\n","            # Building network.\n","            # word_model = WordModel(saved_vocab_file=self.config['saved_vocab_file'],\n","            #                        embed_size=self.config['vocab_embed_size'],\n","            #                        filename=self.config['embed_file'],\n","            #                        embed_type=self.config['embed_type'],\n","            #                        top_n=self.config['top_vocab'],\n","            #                        additional_vocab=vocab)\n","            word_model = WordModel(embed_size=self.config['vocab_embed_size'],\n","                                   filename=self.config['embed_file'],\n","                                   embed_type=self.config['embed_type'],\n","                                   top_n=self.config['top_vocab'],\n","                                   additional_vocab=vocab)\n","            self._init_new_network(train_set, word_model)\n","\n","        num_params = 0\n","        for name, p in self.network.named_parameters():\n","            print('{}: {}'.format(name, str(p.size())))\n","            num_params += p.numel()\n","\n","        if self.config['use_bert'] and self.config.get('finetune_bert', None):\n","            for name, p in self.config['bert_model'].named_parameters():\n","                print('{}: {}'.format(name, str(p.size())))\n","                num_params += p.numel()\n","        print('#Parameters = {}\\n'.format(num_params))\n","\n","        self._init_optimizer()\n","        if self.config['pretrained'] and state_dict_opt:\n","            self.optimizer.load_state_dict(state_dict_opt)\n","\n","\n","    def init_saved_network(self, saved_dir):\n","        _ARGUMENTS = ['vocab_embed_size', 'hidden_size', 'f_qem', 'f_pos', 'f_ner',\n","                      'word_dropout', 'rnn_dropout',\n","                      'ctx_graph_hops', 'ctx_graph_topk',\n","                      'score_unk_threshold', 'score_yes_threshold',\n","                      'score_no_threshold', 'max_answer_len']\n","\n","        # Load all saved fields.\n","        fname = os.path.join(saved_dir, _SAVED_WEIGHTS_FILE)\n","        print('[ Loading saved model %s ]' % fname)\n","        saved_params = torch.load(fname, map_location=lambda storage, loc: storage)\n","        self.word_dict = saved_params['word_dict']\n","        self.feature_dict = saved_params['feature_dict']\n","        self.saved_epoch = saved_params['epoch']\n","        for k, v in self.feature_dict.items():\n","            self.config['num_features_{}'.format(k)] = len(v)\n","        state_dict = saved_params['state_dict']\n","        # for k in _ARGUMENTS:\n","        #     if saved_params['config'][k] != self.config[k]:\n","        #         print('Overwrite {}: {} -> {}'.format(k, self.config[k], saved_params['config'][k]))\n","        #         self.config[k] = saved_params['config'][k]\n","\n","        w_embedding = self._init_embedding(len(self.word_dict) + 1, self.config['vocab_embed_size'])\n","        self.network = GraphFlow(self.config, w_embedding)\n","\n","        # Merge the arguments\n","        if state_dict:\n","            merged_state_dict = self.network.state_dict()\n","            for k, v in state_dict['network'].items():\n","                if k in merged_state_dict:\n","                    merged_state_dict[k] = v\n","            self.network.load_state_dict(merged_state_dict)\n","\n","        if self.config['use_bert'] and self.config.get('finetune_bert', None):\n","            self.config['bert_model'].load_state_dict(state_dict['bert'])\n","\n","        return state_dict['optimizer'] if state_dict else None\n","\n","    def _init_new_network(self, train_set, word_model):\n","        self.feature_dict = self._build_feature_dict(train_set)\n","        for k, v in self.feature_dict.items():\n","            self.config['num_features_{}'.format(k)] = len(v)\n","        self.word_dict = word_model.get_vocab()\n","        w_embedding = self._init_embedding(word_model.vocab_size, self.config['vocab_embed_size'],\n","                                           pretrained_vecs=word_model.get_word_vecs())\n","        self.network = GraphFlow(self.config, w_embedding)\n","\n","    def _init_optimizer(self):\n","        parameters = [p for p in self.network.parameters() if p.requires_grad]\n","        if self.config['use_bert'] and self.config.get('finetune_bert', None):\n","            parameters += [p for p in self.config['bert_model'].parameters() if p.requires_grad]\n","        if self.config['optimizer'] == 'sgd':\n","            self.optimizer = optim.SGD(parameters, self.config['learning_rate'],\n","                                       momentum=self.config['momentum'],\n","                                       weight_decay=self.config['weight_decay'])\n","        elif self.config['optimizer'] == 'adam':\n","            self.optimizer = optim.Adam(parameters, lr=self.config['learning_rate'])\n","        elif self.config['optimizer'] == 'adamax':\n","            self.optimizer = optim.Adamax(parameters, lr=self.config['learning_rate'])\n","        elif self.config['optimizer'] == 'radam':\n","            self.optimizer = RAdam(parameters, lr=self.config['learning_rate'])\n","        else:\n","            raise RuntimeError('Unsupported optimizer: %s' % self.config['optimizer'])\n","        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='max', factor=0.5, \\\n","                    patience=1, verbose=True)\n","\n","    def _init_embedding(self, vocab_size, embed_size, pretrained_vecs=None):\n","        \"\"\"Initializes the embeddings\n","        \"\"\"\n","        return nn.Embedding(vocab_size, embed_size, padding_idx=0,\n","                            _weight=torch.from_numpy(pretrained_vecs).float()\n","                            if pretrained_vecs is not None else None)\n","\n","    def _build_feature_dict(self, train_set):\n","        feature_dict = {}\n","        if self.config['f_qem']:\n","            feature_dict['f_qem'] = {'yes': 0, 'no': 1}\n","\n","        if self.config['f_pos'] or self.config['f_ner']:\n","            pos_tags = set([_UNK_POS])\n","            ner_tags = set([_UNK_NER])\n","            for ex in train_set:\n","                if self.config['f_pos']:\n","                    assert 'pos' in ex['evidence']\n","                    pos_tags |= set(ex['evidence']['pos'])\n","                if self.config['f_ner']:\n","                    assert 'ner' in ex['evidence']\n","                    ner_tags |= set(ex['evidence']['ner'])\n","            if self.config['f_pos']:\n","                print('{} pos tags: {}'.format(len(pos_tags), str(pos_tags)))\n","                feature_dict['f_pos'] = dict(zip(pos_tags, range(len(pos_tags))))\n","            if self.config['f_ner']:\n","                print('{} ner tags: {}'.format(len(ner_tags), str(ner_tags)))\n","                feature_dict['f_ner'] = dict(zip(ner_tags, range(len(ner_tags))))\n","        return feature_dict\n","\n","    def compute_span_loss(self, score_s, score_e, targets, target_mask):\n","        assert targets.size(0) == score_s.size(0) == score_e.size(0)\n","        loss = F.nll_loss(score_s.view(-1, score_s.size(-1)), targets[:, :, 0].view(-1), reduction='none') + F.nll_loss(score_e.view(-1, score_e.size(-1)), targets[:, :, 1].view(-1), reduction='none')\n","        loss = torch.sum(loss.view_as(target_mask) * target_mask) / torch.clamp(torch.sum(target_mask), min=VERY_SMALL_NUMBER)\n","        return loss\n","\n","    def save(self, dirname, epoch):\n","        params = {\n","            'state_dict': {\n","                'network': self.network.state_dict(),\n","                'bert': self.config['bert_model'].state_dict() if self.config['use_bert'] and self.config.get('finetune_bert', None) else None,\n","                'optimizer': self.optimizer.state_dict()\n","            },\n","            'word_dict': self.word_dict,\n","            'feature_dict': self.feature_dict,\n","            'config': self.config,\n","            'dir': dirname,\n","            'epoch': epoch\n","        }\n","        try:\n","            torch.save(params, os.path.join(dirname, _SAVED_WEIGHTS_FILE))\n","        except BaseException:\n","            print('[ WARN: Saving failed... continuing anyway. ]')\n","\n","    @abc.abstractmethod\n","    def predict(self, ex, update=True, out_predictions=False):\n","        return\n","\n","    @abc.abstractmethod\n","    def compute_answer_type_loss(self, score_c, answer_type_targets, turn_mask):\n","        return\n","\n","    @abc.abstractmethod\n","    def extract_predictions(self, ex, score_s, score_e, score_c, turn_mask):\n","        return\n","\n","    def _scores_to_text(self, text, score_s, score_e):\n","        max_len = self.config['max_answer_len'] or score_s.size(0)\n","        scores = torch.ger(score_s, score_e)\n","        scores.triu_().tril_(max_len - 1)\n","        scores = scores.cpu().detach().numpy()\n","        s_idx, e_idx = np.unravel_index(np.argmax(scores), scores.shape)\n","        return ' '.join(text[s_idx: e_idx + 1]), (int(s_idx), int(e_idx))\n","\n","    def _scores_to_raw_text(self, raw_text, offsets, score_s, score_e):\n","        max_len = self.config['max_answer_len'] or score_s.size(0)\n","        scores = torch.ger(score_s, score_e)\n","        scores.triu_().tril_(max_len - 1)\n","        scores = scores.cpu().detach().numpy()\n","        s_idx, e_idx = np.unravel_index(np.argmax(scores), scores.shape)\n","        prediction = raw_text[offsets[s_idx][0]: offsets[e_idx][1]]\n","        span = (offsets[s_idx][0], offsets[e_idx][1])\n","        return prediction, span\n","\n","class CoQAModel(Model):\n","    \"\"\"High level CoQA model that handles intializing the underlying network\n","    architecture, saving, updating examples, and predicting examples.\n","    \"\"\"\n","\n","    def __init__(self, config, train_set=None):\n","        super(CoQAModel, self).__init__(config, train_set)\n","\n","\n","    def predict(self, ex, step, update=True, out_predictions=False):\n","        # Train/Eval mode\n","        self.network.train(update)\n","        # Run forward\n","        with torch.set_grad_enabled(update):\n","            res = self.network(ex)\n","        score_s, score_e, score_c = res['start_logits'], res['end_logits'], res['score_c']\n","\n","        output = {\n","            'metrics': {'f1': 0.0, 'em': 0.0},\n","            'loss': 0.0,\n","            'total_qs': 0,\n","            'total_dials': 0\n","        }\n","\n","\n","        # Compute loss\n","        loss = self.compute_span_loss(score_s, score_e, ex['targets'], ex['span_mask'])\n","        loss = loss + self.compute_answer_type_loss(score_c, ex['answer_type_targets'], res['turn_mask'])\n","        output['loss'] = loss.item()\n","\n","        if update:\n","            # Accumulate gradients\n","            loss = loss / self.config['grad_accumulated_steps'] # Normalize our loss (if averaged)\n","            # Run backward\n","            loss.backward()\n","\n","            if (step + 1) % self.config['grad_accumulated_steps'] == 0: # Wait for several backward steps\n","                if self.config['optimizer'] != 'bert_adam' and self.config['grad_clipping']:\n","                    # Clip gradients\n","                    parameters = [p for p in self.network.parameters() if p.requires_grad]\n","                    if self.config['use_bert'] and self.config.get('finetune_bert', None):\n","                        parameters += [p for p in self.config['bert_model'].parameters() if p.requires_grad]\n","\n","                    torch.nn.utils.clip_grad_norm_(parameters, self.config['grad_clipping'])\n","\n","                # Update parameters\n","                self.optimizer.step()\n","                self.optimizer.zero_grad()\n","\n","        if (not update) or self.config['predict_train']:\n","            predictions, spans = self.extract_predictions(ex, score_s, score_e, score_c, res['turn_mask'])\n","            expand_predictions = [y for x in predictions for y in x]\n","            expand_answers = [y for x in ex['answers'] for y in x]\n","            output['total_qs'] = len(expand_predictions)\n","            output['total_dials'] = len(predictions)\n","            f1, em = self.evaluate_predictions(expand_predictions, expand_answers)\n","            output['metrics']['f1'] = f1\n","            output['metrics']['em'] = em\n","            if out_predictions:\n","                output['predictions'] = predictions\n","                output['spans'] = spans\n","        torch.cuda.empty_cache()\n","        return output\n","\n","    def compute_answer_type_loss(self, score_c, answer_type_targets, turn_mask):\n","        loss = F.cross_entropy(score_c.view(-1, score_c.size(-1)), answer_type_targets.view(-1), reduction='none')\n","        loss = torch.sum(loss.view_as(turn_mask) * turn_mask) / torch.clamp(torch.sum(turn_mask), min=VERY_SMALL_NUMBER)\n","        return loss\n","\n","    def extract_predictions(self, ex, score_s, score_e, score_c, turn_mask):\n","        # Transfer to CPU/normal tensors for numpy ops (and convert log probabilities to probabilities)\n","        score_s = score_s.exp()\n","        score_e = score_e.exp()\n","        score_c = score_c.data.cpu()\n","\n","        predictions = []\n","        spans = []\n","        for i, (_s, _e, _c) in enumerate(zip(score_s, score_e, score_c)): # Example-level\n","            para_pred = []\n","            para_span = []\n","            for j in range(_s.size(0)): # Turn-level\n","                if turn_mask[i, j] == 0: # This dialog has ended\n","                    break\n","\n","                ans_type = np.argmax(_c[j]).item()\n","                if ans_type == CoQA_UNK_ANSWER_LABEL:\n","                    pred = CoQA_UNK_ANSWER\n","                    span = (-1, -1)\n","                elif ans_type == CoQA_ANSWER_YES_LABEL:\n","                    pred = CoQA_YES_ANSWER\n","                    span = (-1, -1)\n","                elif ans_type == CoQA_ANSWER_NO_LABEL:\n","                    pred = CoQA_NO_ANSWER\n","                    span = (-1, -1)\n","                else:\n","                    if self.config['predict_raw_text']:\n","                        pred, span = self._scores_to_raw_text(ex['raw_evidence_text'][i],\n","                                                                    ex['offsets'][i], _s[j], _e[j])\n","                    else:\n","                        pred, span = self._scores_to_text(ex['evidence_text'][i], _s[j], _e[j])\n","\n","                para_pred.append(pred)\n","                para_span.append(span)\n","            predictions.append(para_pred)\n","            spans.append(para_span)\n","        return predictions, spans\n","\n","    def evaluate_predictions(self, predictions, answers):\n","        assert len(predictions) == len(answers)\n","        f1_score = compute_eval_metric('f1', predictions, answers)\n","        em_score = compute_eval_metric('em', predictions, answers)\n","        return f1_score, em_score\n","\n","class QuACModel(Model):\n","    \"\"\"High level QuAC model that handles intializing the underlying network\n","    architecture, saving, updating examples, and predicting examples.\n","    \"\"\"\n","\n","    def __init__(self, config, train_set=None):\n","        super(QuACModel, self).__init__(config, train_set)\n","        self.eval_fn = quac_eval_fn if config['dataset_name'] == 'quac' else doqa_eval_fn\n","\n","    def predict(self, ex, step, update=True, out_predictions=False):\n","        # Train/Eval mode\n","        self.network.train(update)\n","        # Run forward\n","        with torch.set_grad_enabled(update):\n","            res = self.network(ex)\n","        score_s, score_e, unk_probs, score_yesno, score_followup = res['start_logits'], res['end_logits'], res['unk_probs'], res['score_yesno'], res['score_followup']\n","\n","        output = {\n","            'metrics': None,\n","            'loss': 0.0,\n","            'total_qs': 0,\n","            'total_dials': 0\n","        }\n","\n","        # Compute loss\n","        loss = self.compute_span_loss(score_s, score_e, ex['targets'], ex['span_mask'])\n","        loss = loss + self.compute_answer_type_loss(unk_probs, score_yesno, score_followup, ex['unk_answer_targets'], ex['yesno_targets'], ex['followup_targets'], res['turn_mask'])\n","        output['loss'] = loss.item()\n","\n","        if update:\n","            # Accumulate gradients\n","            loss = loss / self.config['grad_accumulated_steps'] # Normalize our loss (if averaged)\n","            # Run backward\n","            loss.backward()\n","\n","            if (step + 1) % self.config['grad_accumulated_steps'] == 0: # Wait for several backward steps\n","                if self.config['optimizer'] != 'bert_adam' and self.config['grad_clipping']:\n","                    # Clip gradients\n","                    parameters = [p for p in self.network.parameters() if p.requires_grad]\n","                    if self.config['use_bert'] and self.config.get('finetune_bert', None):\n","                        parameters += [p for p in self.config['bert_model'].parameters() if p.requires_grad]\n","\n","                    torch.nn.utils.clip_grad_norm_(parameters, self.config['grad_clipping'])\n","\n","                # Update parameters\n","                self.optimizer.step()\n","                self.optimizer.zero_grad()\n","\n","        if (not update) or self.config['predict_train']:\n","            predictions, spans, yesnos, followups = self.extract_predictions(ex, score_s, score_e, unk_probs, score_yesno, score_followup, self.config['unk_answer_threshold'], res['turn_mask'])\n","            output['metrics'], total_qs, total_dials = self.eval_fn(ex['answers'], predictions, ex['raw_evidence_text'])\n","            output['total_qs'] = total_qs\n","            output['total_dials'] = total_dials\n","\n","            if out_predictions:\n","                output['predictions'] = predictions\n","                output['spans'] = spans\n","                output['yesnos'] = yesnos\n","                output['followups'] = followups\n","        torch.cuda.empty_cache()\n","        return output\n","\n","    def compute_answer_type_loss(self, unk_probs, score_yesno, score_followup, unk_answer_targets, yesno_targets, followup_targets, turn_mask):\n","        loss = F.binary_cross_entropy(unk_probs.view(-1), unk_answer_targets.view(-1), reduction='none') \\\n","                + F.cross_entropy(score_yesno.view(-1, score_yesno.size(-1)), yesno_targets.view(-1), reduction='none') \\\n","                + F.cross_entropy(score_followup.view(-1, score_followup.size(-1)), followup_targets.view(-1), reduction='none')\n","        loss = torch.sum(loss.view_as(turn_mask) * turn_mask) / torch.clamp(torch.sum(turn_mask), min=VERY_SMALL_NUMBER)\n","        return loss\n","\n","    def extract_predictions(self, ex, score_s, score_e, unk_probs, score_yesno, score_followup, unk_answer_threshold, turn_mask):\n","        # Transfer to CPU/normal tensors for numpy ops (and convert log probabilities to probabilities)\n","        score_s = score_s.exp()\n","        score_e = score_e.exp()\n","        score_yesno = score_yesno.data.cpu()\n","        score_followup = score_followup.data.cpu()\n","\n","\n","        predictions = []\n","        spans = []\n","        yesnos = []\n","        followups = []\n","        for i, (_s, _e, _unk, _yesno, _followup) in enumerate(zip(score_s, score_e, unk_probs, score_yesno, score_followup)): # Example-level\n","            para_pred = []\n","            para_span = []\n","            para_yesno = []\n","            para_followup = []\n","            for j in range(_s.size(0)): # Turn-level\n","                if turn_mask[i, j] == 0: # This dialog has ended\n","                    break\n","\n","                if _unk[j].item() >= unk_answer_threshold:\n","                    pred = QuAC_UNK_ANSWER.upper()\n","                    span = (-1, -1)\n","                else:\n","                    if self.config['predict_raw_text']:\n","                        pred, span = self._scores_to_raw_text(ex['raw_evidence_text'][i],\n","                                                                    ex['offsets'][i], _s[j], _e[j])\n","                    else:\n","                        pred, span = self._scores_to_text(ex['evidence_text'][i], _s[j], _e[j])\n","\n","                yesno_type = np.argmax(_yesno[j]).item()\n","                if yesno_type == QuAC_YESNO_YES_LABEL:\n","                    yesno = QuAC_YESNO_YES\n","                elif yesno_type == QuAC_YESNO_NO_LABEL:\n","                    yesno = QuAC_YESNO_NO\n","                else:\n","                    yesno = QuAC_YESNO_OTHER\n","\n","                followup_type = np.argmax(_followup[j]).item()\n","\n","                if self.config['dataset_name'] == 'quac':\n","                    if followup_type == QuAC_FOLLOWUP_YES_LABEL:\n","                        followup = QuAC_FOLLOWUP_YES\n","                    elif followup_type == QuAC_FOLLOWUP_NO_LABEL:\n","                        followup = QuAC_FOLLOWUP_NO\n","                    else:\n","                        followup = QuAC_FOLLOWUP_OTHER\n","\n","                else:\n","                    if followup_type == DoQA_FOLLOWUP_YES_LABEL:\n","                        followup = DoQA_FOLLOWUP_YES\n","                    else:\n","                        followup = DoQA_FOLLOWUP_NO\n","\n","                para_pred.append(pred)\n","                para_span.append(span)\n","                para_yesno.append(yesno)\n","                para_followup.append(followup)\n","            predictions.append(para_pred)\n","            spans.append(para_span)\n","            yesnos.append(para_yesno)\n","            followups.append(para_followup)\n","        return predictions, spans, yesnos, followups"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ckga1F6v6Dz"},"source":["## Model handler "]},{"cell_type":"code","metadata":{"id":"GZUx4wpNv5bV","executionInfo":{"status":"ok","timestamp":1610352658802,"user_tz":-420,"elapsed":94265,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}}},"source":["import os\n","import time\n","import json\n","\n","import torch\n","from torch.utils.data import DataLoader\n","import torch.backends.cudnn as cudnn\n","\n","# from .model import CoQAModel, QuACModel\n","# from .utils import prepare_datasets, sanitize_input, vectorize_input\n","# from .utils import Timer, DummyLogger, AverageMeter\n","# from .utils import  as \n","\n","\n","class ModelHandler(object):\n","    \"\"\"High level model_handler that trains/validates/tests the network,\n","    tracks and logs metrics.\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        config['dataset_name'] = config['dataset_name'].lower()\n","        if config['dataset_name'] == 'coqa':\n","            QAModel = CoQAModel\n","            # Evaluation Metrics:\n","            self._train_loss = AverageMeter()\n","            self._dev_loss = AverageMeter()\n","            self._train_metrics = {'f1': AverageMeter(),\n","                                'em': AverageMeter()}\n","            self._dev_metrics = {'f1': AverageMeter(),\n","                                'em': AverageMeter()}\n","            config['coqa_answer_class_num'] =CoQA_ANSWER_CLASS_NUM\n","\n","        elif config['dataset_name'] in ('quac', 'doqa'):\n","            QAModel = QuACModel\n","            # Evaluation Metrics:\n","            self._train_loss = AverageMeter()\n","            self._dev_loss = AverageMeter()\n","            self._train_metrics = {'f1': AverageMeter(),\n","                                'heq': AverageMeter(),\n","                                'dheq': AverageMeter()}\n","            self._dev_metrics = {'f1': AverageMeter(),\n","                                'heq': AverageMeter(),\n","                                'dheq': AverageMeter()}\n","\n","            config['quac_yesno_class_num'] =QuAC_YESNO_CLASS_NUM\n","            if config['dataset_name'] == 'quac':\n","                config['quac_followup_class_num'] =QuAC_FOLLOWUP_CLASS_NUM\n","            else:\n","                config['quac_followup_class_num'] =DoQA_FOLLOWUP_CLASS_NUM\n","\n","        else:\n","            raise ValueError('Unknown dataset name: {}'.format(config['dataset_name']))\n","\n","\n","        self.logger = DummyLogger(config, dirname=config['out_dir'], pretrained=config['pretrained'])\n","        self.dirname = self.logger.dirname\n","        if not config['no_cuda'] and torch.cuda.is_available():\n","            print('[ Using CUDA ]')\n","            self.device = torch.device('cuda' if config['cuda_id'] < 0 else 'cuda:%d' % config['cuda_id'])\n","            cudnn.benchmark = True\n","        else:\n","            self.device = torch.device('cpu')\n","        config['device'] = self.device\n","\n","        # Prepare datasets\n","        datasets = prepare_datasets(config)\n","        train_set = datasets['train']\n","        dev_set = datasets['dev']\n","        test_set = datasets['test']\n","\n","        if train_set:\n","            self.train_loader = DataLoader(train_set, batch_size=config['batch_size'],\n","                                           shuffle=config['shuffle'], collate_fn=lambda x: x, pin_memory=True)\n","            self._n_train_batches = len(train_set) // config['batch_size']\n","        else:\n","            self.train_loader = None\n","\n","        if dev_set:\n","            self.dev_loader = DataLoader(dev_set, batch_size=config['batch_size'],\n","                                         shuffle=False, collate_fn=lambda x: x, pin_memory=True)\n","            self._n_dev_batches = len(dev_set) // config['batch_size']\n","        else:\n","            self.dev_loader = None\n","\n","        if test_set:\n","            self.test_loader = DataLoader(test_set, batch_size=config['batch_size'], shuffle=False,\n","                                          collate_fn=lambda x: x, pin_memory=True)\n","            self._n_test_batches = len(test_set) // config['batch_size']\n","            self._n_test_examples = len(test_set)\n","        else:\n","            self.test_loader = None\n","\n","        # Load BERT featrues\n","        if config['use_bert']:\n","            from pytorch_pretrained_bert import BertTokenizer\n","            from pytorch_pretrained_bert.modeling import BertModel\n","            print('[ Using pretrained BERT features ]')\n","            self.bert_tokenizer = BertTokenizer.from_pretrained(config['bert_model'], do_lower_case=True)\n","            self.bert_model = BertModel.from_pretrained(config['bert_model']).to(self.device)\n","            config['bert_model'] = self.bert_model\n","            if not config.get('finetune_bert', None):\n","                print('[ Fix BERT layers ]')\n","                self.bert_model.eval()\n","                for param in self.bert_model.parameters():\n","                    param.requires_grad = False\n","            else:\n","                print('[ Finetune BERT layers ]')\n","        else:\n","            self.bert_tokenizer = None\n","            self.bert_model = None\n","\n","        # Initialize the QA model\n","        self._n_train_examples = 0\n","        self.model = QAModel(config, train_set)\n","        self.model.network = self.model.network.to(self.device)\n","        self.config = self.model.config\n","        self.is_test = False\n","\n","    def train(self):\n","        if self.train_loader is None or self.dev_loader is None:\n","            print(\"No training set or dev set specified -- skipped training.\")\n","            return\n","\n","        self.is_test = False\n","        timer = Timer(\"Train\")\n","        if self.config['pretrained']:\n","            self._epoch = self._best_epoch = self.model.saved_epoch\n","        else:\n","            self._epoch = self._best_epoch = 0\n","\n","        self._best_metrics = {}\n","        for k in self._dev_metrics:\n","            self._best_metrics[k] = self._dev_metrics[k].mean()\n","        self._reset_metrics()\n","\n","        while self._stop_condition(self._epoch, self.config['patience']):\n","            self._epoch += 1\n","\n","            print(\"\\n>>> Train Epoch: [{} / {}]\".format(self._epoch, self.config['max_epochs']))\n","            self.logger.write_to_file(\"\\n>>> Train Epoch: [{} / {}]\".format(self._epoch, self.config['max_epochs']))\n","            self._run_epoch(self.train_loader, training=True, verbose=self.config['verbose'])\n","            train_epoch_time = timer.interval(\"Training Epoch {}\".format(self._epoch))\n","            format_str = \"Training Epoch {} -- Loss: {:0.4f}\".format(self._epoch, self._train_loss.mean())\n","            format_str += self.metric_to_str(self._train_metrics)\n","            self.logger.write_to_file(format_str)\n","            print(format_str)\n","\n","            print(\"\\n>>> Dev Epoch: [{} / {}]\".format(self._epoch, self.config['max_epochs']))\n","            self.logger.write_to_file(\"\\n>>> Dev Epoch: [{} / {}]\".format(self._epoch, self.config['max_epochs']))\n","            self._run_epoch(self.dev_loader, training=False, verbose=self.config['verbose'])\n","            timer.interval(\"Validation Epoch {}\".format(self._epoch))\n","            format_str = \"Validation Epoch {} -- Loss: {:0.4f}\".format(self._epoch, self._dev_loss.mean())\n","            format_str += self.metric_to_str(self._dev_metrics)\n","            self.logger.write_to_file(format_str)\n","            print(format_str)\n","\n","            early_stop_metric = self.config.get('early_stop_metric', 'f1')\n","            self.model.scheduler.step(self._dev_metrics[early_stop_metric].mean())\n","            if self._best_metrics[early_stop_metric] <= self._dev_metrics[early_stop_metric].mean():  # Can be one of loss, f1, or em.\n","                self._best_epoch = self._epoch\n","                for k in self._dev_metrics:\n","                    self._best_metrics[k] = self._dev_metrics[k].mean()\n","\n","                if self.config['save_params']:\n","                    self.model.save(self.dirname, self._epoch)\n","                    print('Saved model to {}'.format(self.dirname))\n","                format_str = \"!!! Updated: \" + self.best_metric_to_str(self._best_metrics)\n","                self.logger.write_to_file(format_str)\n","                print(format_str)\n","\n","            self._reset_metrics()\n","\n","        timer.finish()\n","        self.training_time = timer.total\n","\n","        print(\"Finished Training: {}\".format(self.dirname))\n","        print(self.summary())\n","        return self._best_metrics\n","\n","    def test(self):\n","        if self.test_loader is None:\n","            print(\"No testing set specified -- skipped testing.\")\n","            return\n","\n","        # Restore best model\n","        print('Restoring best model')\n","        self.model.init_saved_network(self.dirname)\n","        self.model.network = self.model.network.to(self.device)\n","\n","\n","        self.is_test = True\n","        self._reset_metrics()\n","        timer = Timer(\"Test\")\n","        output = self._run_epoch(self.test_loader, training=False, verbose=0,\n","                                 out_predictions=self.config['out_predictions'])\n","\n","        if self.config['out_predictions']:\n","            if self.config['out_pred_in_folder']:\n","                output_file = os.path.join(self.dirname,_PREDICTION_FILE)\n","            else:\n","                output_file =_PREDICTION_FILE\n","            with open(output_file, 'w', encoding='utf8') as outfile:\n","                if self.config['dataset_name'] == 'coqa':\n","                    json.dump(output, outfile, indent=4, ensure_ascii=False)\n","                else:\n","                    for pred in output:\n","                        outfile.write(json.dumps(pred) + '\\n')\n","\n","        timer.finish()\n","        print(self.self_report(self._n_test_batches, 'test'))\n","        print(\"Finished Testing: {}\".format(self.dirname))\n","        self.logger.close()\n","\n","        test_metrics = {}\n","        for k in self._dev_metrics:\n","            test_metrics[k] = self._dev_metrics[k].mean()\n","        return test_metrics\n","\n","    def _run_epoch(self, data_loader, training=True, verbose=10, out_predictions=False):\n","        start_time = time.time()\n","        if training:\n","            self.model.optimizer.zero_grad()\n","        output = []\n","        for step, input_batch in enumerate(data_loader):\n","            input_batch = sanitize_input(input_batch, self.config, self.model.word_dict,\n","                                         self.model.feature_dict, self.bert_tokenizer, training=training)\n","            x_batch = vectorize_input(input_batch, self.config, self.bert_model, training=training, device=self.device)\n","            if not x_batch:\n","                continue  # When there are no target spans present in the batch\n","\n","            res = self.model.predict(x_batch, step, update=training, out_predictions=out_predictions)\n","\n","            loss = res['loss']\n","            metrics = res['metrics']\n","            self._update_metrics(loss, metrics, res['total_qs'], res['total_dials'], training=training)\n","\n","            if training:\n","                self._n_train_examples += x_batch['batch_size']\n","\n","            if (verbose > 0) and (step > 0) and (step % verbose == 0):\n","                mode = \"train\" if training else (\"test\" if self.is_test else \"dev\")\n","                summary_str = self.self_report(step, mode)\n","                self.logger.write_to_file(summary_str)\n","                print(summary_str)\n","                print('used_time: {:0.2f}s'.format(time.time() - start_time))\n","\n","            if out_predictions:\n","                if self.config['dataset_name'] == 'coqa':\n","                    for idx, (id, turn_ids) in enumerate(zip(input_batch['id'], input_batch['turn_ids'])):\n","                        for t_idx, t_id in enumerate(turn_ids):\n","                            output.append({'id': id,\n","                                           'turn_id': t_id,\n","                                           'answer': res['predictions'][idx][t_idx]})\n","                else:\n","                    for idx, turn_ids in enumerate(input_batch['turn_ids']):\n","                        qid_list = []\n","                        best_span_str_list = []\n","                        yesno_list = []\n","                        followup_list = []\n","                        for t_idx, t_id in enumerate(turn_ids):\n","                            qid_list.append(t_id)\n","                            best_span_str_list.append(res['predictions'][idx][t_idx])\n","                            yesno_list.append(res['yesnos'][idx][t_idx])\n","                            followup_list.append(res['followups'][idx][t_idx])\n","\n","                        output.append({'qid': qid_list,\n","                                       'best_span_str': best_span_str_list,\n","                                       'yesno': yesno_list,\n","                                       'followup': followup_list})\n","        return output\n","\n","    def self_report(self, step, mode='train'):\n","        if mode == \"train\":\n","            format_str = \"[train-{}] step: [{} / {}] | loss = {:0.4f}\".format(\n","                self._epoch, step, self._n_train_batches, self._train_loss.mean())\n","            format_str += self.metric_to_str(self._train_metrics)\n","        elif mode == \"dev\":\n","            format_str = \"[predict-{}] step: [{} / {}] | loss = {:0.4f}\".format(\n","                    self._epoch, step, self._n_dev_batches, self._dev_loss.mean())\n","            format_str += self.metric_to_str(self._dev_metrics)\n","        elif mode == \"test\":\n","            format_str = \"[test] | test_exs = {} | step: [{} / {}]\".format(\n","                    self._n_test_examples, step, self._n_test_batches)\n","            format_str += self.metric_to_str(self._dev_metrics)\n","        else:\n","            raise ValueError('mode = {} not supported.' % mode)\n","        return format_str\n","\n","    def metric_to_str(self, metrics):\n","        format_str = ''\n","        for k in metrics:\n","            format_str += ' | {} = {:0.2f}'.format(k.upper(), metrics[k].mean())\n","        return format_str\n","\n","    def best_metric_to_str(self, metrics):\n","        format_str = '\\n'\n","        for k in metrics:\n","            format_str += '{} = {:0.2f}\\n'.format(k.upper(), metrics[k])\n","        return format_str\n","\n","    def summary(self):\n","        start = \"\\n<<<<<<<<<<<<<<<< MODEL SUMMARY >>>>>>>>>>>>>>>> \"\n","        info = \"Best epoch = {}; \".format(self._best_epoch) + self.best_metric_to_str(self._best_metrics)\n","        end = \" <<<<<<<<<<<<<<<< MODEL SUMMARY >>>>>>>>>>>>>>>> \"\n","        return \"\\n\".join([start, info, end])\n","\n","    def _update_metrics(self, loss, metrics, total_qs, total_dials, training=True):\n","        if training:\n","            self._train_loss.update(loss)\n","            for k in self._train_metrics:\n","                self._train_metrics[k].update(metrics[k] * 100, total_qs if k.lower() != 'dheq' else total_dials)\n","        else:\n","            self._dev_loss.update(loss)\n","            for k in self._dev_metrics:\n","                self._dev_metrics[k].update(metrics[k] * 100, total_qs if k.lower() != 'dheq' else total_dials)\n","\n","    def _reset_metrics(self):\n","        self._train_loss.reset()\n","        self._dev_loss.reset()\n","\n","        for k in self._train_metrics:\n","            self._train_metrics[k].reset()\n","        for k in self._dev_metrics:\n","            self._dev_metrics[k].reset()\n","\n","    def _stop_condition(self, epoch, patience=10):\n","        \"\"\"\n","        Checks have not exceeded max epochs and has not gone patience epochs without improvement.\n","        \"\"\"\n","        no_improvement = epoch >= self._best_epoch + patience\n","        exceeded_max_epochs = epoch >= self.config['max_epochs']\n","        return False if exceeded_max_epochs or no_improvement else True"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecM4G1KpsZ_h"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"6lN26YYpsZmy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610363406851,"user_tz":-420,"elapsed":10842311,"user":{"displayName":"Sơn Lưu Thanh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1mVQJEgki08d2yAQnpkb4_RIo8FgUquFtRM_PsA=s64","userId":"09824077883060402796"}},"outputId":"b10fff40-ec24-4cb3-d4ad-cb81434c1671"},"source":["import argparse\n","import yaml\n","import torch\n","import numpy as np\n","\n","# from core.model_handler import ModelHandler\n","\n","################################################################################\n","# Main #\n","################################################################################\n","\n","\n","def set_random_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","\n","\n","def main(config):\n","    print_config(config)\n","    set_random_seed(config['random_seed'])\n","    model = ModelHandler(config)\n","    model.train()\n","    model.test()\n","\n","################################################################################\n","# ArgParse and Helper Functions #\n","################################################################################\n","def get_config(config_path=\"config.yml\"):\n","    with open(config_path, \"r\") as setting:\n","        config = yaml.load(setting, Loader=yaml.FullLoader)\n","    return config\n","\n","def get_args():\n","    parser = argparse.ArgumentParser()\n","    # parser.add_argument('-config', '--config', required=True, type=str, help='path to the config file')\n","    parser.add_argument('-f')\n","    args = vars(parser.parse_args())\n","    return args\n","\n","def str2bool(v):\n","    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        raise argparse.ArgumentTypeError('Boolean value expected.')\n","\n","\n","def print_config(config):\n","    print(\"**************** MODEL CONFIGURATION ****************\")\n","    for key in sorted(config.keys()):\n","        val = config[key]\n","        keystr = \"{}\".format(key) + (\" \" * (24 - len(key)))\n","        print(\"{} -->   {}\".format(keystr, val))\n","    print(\"**************** MODEL CONFIGURATION ****************\")\n","\n","################################################################################\n","# Module Command-line Behavior #\n","################################################################################\n","\n","\n","if __name__ == '__main__':\n","    cfg = get_args()\n","    # config = get_config(config)\n","    main(config)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["**************** MODEL CONFIGURATION ****************\n","answer_marker_embed_dim  -->   10\n","batch_size               -->   1\n","bert_dim                 -->   1024\n","bert_doc_stride          -->   250\n","bert_dropout             -->   0.4\n","bert_layer_indexes       -->   [0, 24]\n","bert_max_seq_len         -->   500\n","bert_model               -->   bert-large-uncased\n","bignn                    -->   False\n","ctx_exact_match_embed_dim -->   3\n","ctx_graph_hops           -->   5\n","ctx_graph_topk           -->   10\n","ctx_ner_embed_dim        -->   8\n","ctx_pos_embed_dim        -->   12\n","cuda_id                  -->   0\n","dataset_name             -->   coqa\n","devset                   -->   drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-dev.json\n","embed_file               -->   drive/MyDrive/CODE/CMRC/embedding/wiki.vi.vec\n","embed_type               -->   fasttext\n","f_ner                    -->   False\n","f_pos                    -->   False\n","f_qem                    -->   True\n","f_tf                     -->   False\n","finetune_bert            -->   False\n","fix_vocab_embed          -->   True\n","grad_accumulated_steps   -->   1\n","grad_clipping            -->   10\n","graph_learner_num_pers   -->   1\n","hidden_size              -->   300\n","learning_rate            -->   0.0005\n","logging                  -->   True\n","max_answer_len           -->   12\n","max_epochs               -->   30\n","max_position_distance    -->   160\n","max_turn_num             -->   50\n","min_freq                 -->   5\n","n_history                -->   5\n","n_spatial_kernels        -->   2\n","no_cuda                  -->   False\n","no_pre_answer            -->   False\n","no_pre_question          -->   False\n","optimizer                -->   adamax\n","out_dir                  -->   drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","out_pred_in_folder       -->   True\n","out_predictions          -->   True\n","patience                 -->   10\n","position_emb_size        -->   50\n","predict_raw_text         -->   True\n","predict_train            -->   True\n","pretrained               -->   None\n","ques_marker_embed_dim    -->   3\n","ques_turn_marker_embed_dim -->   5\n","random_seed              -->   1234\n","rnn_dropout              -->   0.3\n","rnn_input_dropout        -->   None\n","save_params              -->   True\n","saved_vocab_file         -->   drive/MyDrive/CODE/CMRC/data_graphflow/models2/word_model_min_5\n","shuffle                  -->   True\n","static_graph             -->   False\n","temporal_gnn             -->   True\n","test_batch_size          -->   1\n","testset                  -->   drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-test.json\n","top_vocab                -->   200000\n","trainset                 -->   drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-train.json\n","use_bert                 -->   False\n","use_bert_gamma           -->   False\n","use_bert_weight          -->   True\n","use_gnn                  -->   True\n","use_position_enc         -->   False\n","use_ques_marker          -->   True\n","use_spatial_kernels      -->   False\n","verbose                  -->   1000\n","vocab_embed_size         -->   300\n","word_dropout             -->   0.3\n","**************** MODEL CONFIGURATION ****************\n","[ Using CUDA ]\n","<> <> <> Starting Timer [Load drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-train.json] <> <> <>\n","Load 1400 paragraphs.\n","Turn num: avg = 5.0, min = 5, max = 5\n","Paragraph length: avg = 411.8, min = 237, max = 807\n","Question length: avg = 47.8, min = 2, max = 246\n","<> <> <> Finished Timer [Load drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-train.json] <> <> <> Total time elapsed: 0h 00m 04s <> <> <>\n","<> <> <> Starting Timer [Load drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-dev.json] <> <> <>\n","Load 300 paragraphs.\n","Turn num: avg = 5.0, min = 5, max = 5\n","Paragraph length: avg = 409.5, min = 245, max = 841\n","Question length: avg = 47.0, min = 4, max = 190\n","<> <> <> Finished Timer [Load drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-dev.json] <> <> <> Total time elapsed: 0h 00m 01s <> <> <>\n","<> <> <> Starting Timer [Load drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-test.json] <> <> <>\n","Load 300 paragraphs.\n","Turn num: avg = 5.0, min = 5, max = 5\n","Paragraph length: avg = 422.9, min = 243, max = 821\n","Question length: avg = 47.2, min = 3, max = 230\n","<> <> <> Finished Timer [Load drive/MyDrive/CODE/CMRC/data_graphflow/vicoqa-test.json] <> <> <> Total time elapsed: 0h 00m 01s <> <> <>\n","Train vocab: 17615\n","Pruned train vocab: 17185\n","<> <> <> Starting Timer [Load drive/MyDrive/CODE/CMRC/embedding/wiki.vi.vec] <> <> <>\n","Embeddings: vocab = 292168, embed_size = 300\n","<> <> <> Finished Timer [Load drive/MyDrive/CODE/CMRC/embedding/wiki.vi.vec] <> <> <> Total time elapsed: 0h 01m 09s <> <> <>\n","Added word: bác_sĩ (train_freq = 14209)\n","Added word: bệnh_nhân (train_freq = 14145)\n","Added word: có_thể (train_freq = 11831)\n","Added word: bệnh_viện (train_freq = 10524)\n","Added word: ung_thư (train_freq = 9618)\n","Added word: điều_trị (train_freq = 8721)\n","Added word: y_tế (train_freq = 6776)\n","Added word: dinh_dưỡng (train_freq = 6640)\n","Added word: cơ_thể (train_freq = 6481)\n","Added word: sức_khỏe (train_freq = 5685)\n","Added 11788 words to the vocab in total.\n","[ Fix word embeddings ]\n","[ Using bidirectional lstm encoder ]\n","[ Using bidirectional lstm encoder ]\n","[ Using bidirectional lstm encoder ]\n","[ Using lstm encoder ]\n","[ Multi-perspective GraphLearner: 1 ]\n","[ Using 5-hop ContextGraphNN ]\n","[ Using graph type: dynamic ]\n","[ Using 5-hop ContextGraphNN ]\n","[ Using graph type: dynamic ]\n","word_embed.weight: torch.Size([211792, 300])\n","ctx_exact_match_embed.weight: torch.Size([2, 3])\n","ctx_ans_marker_embed.weight: torch.Size([21, 10])\n","ques_marker_embed.weight: torch.Size([11, 3])\n","ques_enc.model.weight_ih_l0: torch.Size([600, 303])\n","ques_enc.model.weight_hh_l0: torch.Size([600, 150])\n","ques_enc.model.bias_ih_l0: torch.Size([600])\n","ques_enc.model.bias_hh_l0: torch.Size([600])\n","ques_enc.model.weight_ih_l0_reverse: torch.Size([600, 303])\n","ques_enc.model.weight_hh_l0_reverse: torch.Size([600, 150])\n","ques_enc.model.bias_ih_l0_reverse: torch.Size([600])\n","ques_enc.model.bias_hh_l0_reverse: torch.Size([600])\n","ctx_enc_l1.model.weight_ih_l0: torch.Size([600, 653])\n","ctx_enc_l1.model.weight_hh_l0: torch.Size([600, 150])\n","ctx_enc_l1.model.bias_ih_l0: torch.Size([600])\n","ctx_enc_l1.model.bias_hh_l0: torch.Size([600])\n","ctx_enc_l1.model.weight_ih_l0_reverse: torch.Size([600, 653])\n","ctx_enc_l1.model.weight_hh_l0_reverse: torch.Size([600, 150])\n","ctx_enc_l1.model.bias_ih_l0_reverse: torch.Size([600])\n","ctx_enc_l1.model.bias_hh_l0_reverse: torch.Size([600])\n","ctx_enc_l2.model.weight_ih_l0: torch.Size([600, 600])\n","ctx_enc_l2.model.weight_hh_l0: torch.Size([600, 150])\n","ctx_enc_l2.model.bias_ih_l0: torch.Size([600])\n","ctx_enc_l2.model.bias_hh_l0: torch.Size([600])\n","ctx_enc_l2.model.weight_ih_l0_reverse: torch.Size([600, 600])\n","ctx_enc_l2.model.weight_hh_l0_reverse: torch.Size([600, 150])\n","ctx_enc_l2.model.bias_ih_l0_reverse: torch.Size([600])\n","ctx_enc_l2.model.bias_hh_l0_reverse: torch.Size([600])\n","rnn_ques_over_time.model.weight_ih_l0: torch.Size([1200, 300])\n","rnn_ques_over_time.model.weight_hh_l0: torch.Size([1200, 300])\n","rnn_ques_over_time.model.bias_ih_l0: torch.Size([1200])\n","rnn_ques_over_time.model.bias_hh_l0: torch.Size([1200])\n","ctx2ques_attn.linear_sim.weight: torch.Size([300, 300])\n","ctx2ques_attn_l2.linear_sim.weight: torch.Size([300, 600])\n","ques_self_atten.W1: torch.Size([300, 300])\n","ques_self_atten.W2: torch.Size([300, 1])\n","graph_learner.weight_tensor: torch.Size([1, 653])\n","ctx_gnn.gru_step.linear_z.weight: torch.Size([300, 600])\n","ctx_gnn.gru_step.linear_r.weight: torch.Size([300, 600])\n","ctx_gnn.gru_step.linear_t.weight: torch.Size([300, 600])\n","ctx_gnn_l2.gru_step.linear_z.weight: torch.Size([300, 600])\n","ctx_gnn_l2.gru_step.linear_r.weight: torch.Size([300, 600])\n","ctx_gnn_l2.gru_step.linear_t.weight: torch.Size([300, 600])\n","graph_gru_step.fc_z.weight: torch.Size([300, 1200])\n","graph_gru_step.fc_z.bias: torch.Size([300])\n","graph_gru_step_l2.fc_z.weight: torch.Size([300, 1200])\n","graph_gru_step_l2.fc_z.bias: torch.Size([300])\n","gru_step.linear_z.weight: torch.Size([300, 600])\n","gru_step.linear_r.weight: torch.Size([300, 600])\n","gru_step.linear_t.weight: torch.Size([300, 600])\n","linear_start.weight: torch.Size([300, 300])\n","linear_end.weight: torch.Size([300, 300])\n","fc_clf.weight: torch.Size([2400, 300])\n","fc_clf.bias: torch.Size([2400])\n","#Parameters = 70278602\n","\n","<> <> <> Starting Timer [Train] <> <> <>\n","\n",">>> Train Epoch: [1 / 30]\n","[train-1] step: [1000 / 1400] | loss = 9.9685 | F1 = 11.75 | EM = 0.72\n","used_time: 233.49s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 1]: 0h 05m 26s <> <>\n","Training Epoch 1 -- Loss: 9.5877 | F1 = 14.47 | EM = 1.46\n","\n",">>> Dev Epoch: [1 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 1]: 0h 00m 23s <> <>\n","Validation Epoch 1 -- Loss: 8.1188 | F1 = 22.47 | EM = 2.27\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 22.47\n","EM = 2.27\n","\n","\n",">>> Train Epoch: [2 / 30]\n","[train-2] step: [1000 / 1400] | loss = 8.0941 | F1 = 24.54 | EM = 3.84\n","used_time: 232.29s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 2]: 0h 05m 26s <> <>\n","Training Epoch 2 -- Loss: 7.9859 | F1 = 25.21 | EM = 4.00\n","\n",">>> Dev Epoch: [2 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 2]: 0h 00m 24s <> <>\n","Validation Epoch 2 -- Loss: 7.2642 | F1 = 22.76 | EM = 2.80\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 22.76\n","EM = 2.80\n","\n","\n",">>> Train Epoch: [3 / 30]\n","[train-3] step: [1000 / 1400] | loss = 7.2539 | F1 = 29.77 | EM = 6.07\n","used_time: 234.24s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 3]: 0h 05m 26s <> <>\n","Training Epoch 3 -- Loss: 7.2108 | F1 = 30.25 | EM = 6.09\n","\n",">>> Dev Epoch: [3 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 3]: 0h 00m 24s <> <>\n","Validation Epoch 3 -- Loss: 6.6047 | F1 = 31.17 | EM = 5.90\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 31.17\n","EM = 5.90\n","\n","\n",">>> Train Epoch: [4 / 30]\n","[train-4] step: [1000 / 1400] | loss = 6.7155 | F1 = 32.71 | EM = 7.21\n","used_time: 232.69s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 4]: 0h 05m 25s <> <>\n","Training Epoch 4 -- Loss: 6.7175 | F1 = 33.24 | EM = 7.57\n","\n",">>> Dev Epoch: [4 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 4]: 0h 00m 24s <> <>\n","Validation Epoch 4 -- Loss: 6.3563 | F1 = 34.00 | EM = 7.57\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 34.00\n","EM = 7.57\n","\n","\n",">>> Train Epoch: [5 / 30]\n","[train-5] step: [1000 / 1400] | loss = 6.3900 | F1 = 36.02 | EM = 9.23\n","used_time: 238.07s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 5]: 0h 05m 30s <> <>\n","Training Epoch 5 -- Loss: 6.3349 | F1 = 36.21 | EM = 9.40\n","\n",">>> Dev Epoch: [5 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 5]: 0h 00m 24s <> <>\n","Validation Epoch 5 -- Loss: 5.9693 | F1 = 36.49 | EM = 8.33\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 36.49\n","EM = 8.33\n","\n","\n",">>> Train Epoch: [6 / 30]\n","[train-6] step: [1000 / 1400] | loss = 6.0519 | F1 = 37.50 | EM = 10.43\n","used_time: 235.96s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 6]: 0h 05m 29s <> <>\n","Training Epoch 6 -- Loss: 6.0274 | F1 = 38.20 | EM = 10.71\n","\n",">>> Dev Epoch: [6 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 6]: 0h 00m 24s <> <>\n","Validation Epoch 6 -- Loss: 5.7283 | F1 = 37.31 | EM = 8.70\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 37.31\n","EM = 8.70\n","\n","\n",">>> Train Epoch: [7 / 30]\n","[train-7] step: [1000 / 1400] | loss = 5.7730 | F1 = 40.05 | EM = 11.69\n","used_time: 234.18s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 7]: 0h 05m 29s <> <>\n","Training Epoch 7 -- Loss: 5.7857 | F1 = 39.54 | EM = 11.43\n","\n",">>> Dev Epoch: [7 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 7]: 0h 00m 24s <> <>\n","Validation Epoch 7 -- Loss: 5.8537 | F1 = 38.48 | EM = 10.97\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 38.48\n","EM = 10.97\n","\n","\n",">>> Train Epoch: [8 / 30]\n","[train-8] step: [1000 / 1400] | loss = 5.5964 | F1 = 41.07 | EM = 12.91\n","used_time: 235.40s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 8]: 0h 05m 29s <> <>\n","Training Epoch 8 -- Loss: 5.5581 | F1 = 41.10 | EM = 12.43\n","\n",">>> Dev Epoch: [8 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 8]: 0h 00m 24s <> <>\n","Validation Epoch 8 -- Loss: 5.5044 | F1 = 37.95 | EM = 9.33\n","\n",">>> Train Epoch: [9 / 30]\n","[train-9] step: [1000 / 1400] | loss = 5.3671 | F1 = 41.79 | EM = 13.01\n","used_time: 231.12s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 9]: 0h 05m 25s <> <>\n","Training Epoch 9 -- Loss: 5.3521 | F1 = 41.88 | EM = 13.13\n","\n",">>> Dev Epoch: [9 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 9]: 0h 00m 24s <> <>\n","Validation Epoch 9 -- Loss: 5.3945 | F1 = 40.85 | EM = 11.57\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 40.85\n","EM = 11.57\n","\n","\n",">>> Train Epoch: [10 / 30]\n","[train-10] step: [1000 / 1400] | loss = 5.1719 | F1 = 42.72 | EM = 13.85\n","used_time: 235.29s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 10]: 0h 05m 30s <> <>\n","Training Epoch 10 -- Loss: 5.1565 | F1 = 43.16 | EM = 13.97\n","\n",">>> Dev Epoch: [10 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 10]: 0h 00m 24s <> <>\n","Validation Epoch 10 -- Loss: 5.4634 | F1 = 42.98 | EM = 12.80\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 42.98\n","EM = 12.80\n","\n","\n",">>> Train Epoch: [11 / 30]\n","[train-11] step: [1000 / 1400] | loss = 5.0221 | F1 = 44.34 | EM = 14.55\n","used_time: 233.02s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 11]: 0h 05m 28s <> <>\n","Training Epoch 11 -- Loss: 4.9962 | F1 = 44.64 | EM = 14.79\n","\n",">>> Dev Epoch: [11 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 11]: 0h 00m 24s <> <>\n","Validation Epoch 11 -- Loss: 5.2238 | F1 = 41.72 | EM = 11.40\n","\n",">>> Train Epoch: [12 / 30]\n","[train-12] step: [1000 / 1400] | loss = 4.8425 | F1 = 45.77 | EM = 16.54\n","used_time: 232.69s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 12]: 0h 05m 25s <> <>\n","Training Epoch 12 -- Loss: 4.8209 | F1 = 45.78 | EM = 16.01\n","\n",">>> Dev Epoch: [12 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 12]: 0h 00m 23s <> <>\n","Validation Epoch 12 -- Loss: 5.1755 | F1 = 41.98 | EM = 13.03\n","Epoch    11: reducing learning rate of group 0 to 2.5000e-04.\n","\n",">>> Train Epoch: [13 / 30]\n","[train-13] step: [1000 / 1400] | loss = 4.4636 | F1 = 46.90 | EM = 16.92\n","used_time: 235.24s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 13]: 0h 05m 28s <> <>\n","Training Epoch 13 -- Loss: 4.4996 | F1 = 46.95 | EM = 16.96\n","\n",">>> Dev Epoch: [13 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 13]: 0h 00m 24s <> <>\n","Validation Epoch 13 -- Loss: 5.1049 | F1 = 43.51 | EM = 13.03\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 43.51\n","EM = 13.03\n","\n","\n",">>> Train Epoch: [14 / 30]\n","[train-14] step: [1000 / 1400] | loss = 4.3788 | F1 = 48.51 | EM = 18.70\n","used_time: 233.65s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 14]: 0h 05m 27s <> <>\n","Training Epoch 14 -- Loss: 4.3833 | F1 = 48.31 | EM = 18.03\n","\n",">>> Dev Epoch: [14 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 14]: 0h 00m 24s <> <>\n","Validation Epoch 14 -- Loss: 5.1854 | F1 = 40.87 | EM = 10.93\n","\n",">>> Train Epoch: [15 / 30]\n","[train-15] step: [1000 / 1400] | loss = 4.3121 | F1 = 47.93 | EM = 17.98\n","used_time: 232.35s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 15]: 0h 05m 26s <> <>\n","Training Epoch 15 -- Loss: 4.2910 | F1 = 48.31 | EM = 17.87\n","\n",">>> Dev Epoch: [15 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 15]: 0h 00m 24s <> <>\n","Validation Epoch 15 -- Loss: 5.2313 | F1 = 41.49 | EM = 11.63\n","Epoch    14: reducing learning rate of group 0 to 1.2500e-04.\n","\n",">>> Train Epoch: [16 / 30]\n","[train-16] step: [1000 / 1400] | loss = 4.1190 | F1 = 49.55 | EM = 19.98\n","used_time: 234.14s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 16]: 0h 05m 28s <> <>\n","Training Epoch 16 -- Loss: 4.0987 | F1 = 49.94 | EM = 19.67\n","\n",">>> Dev Epoch: [16 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 16]: 0h 00m 24s <> <>\n","Validation Epoch 16 -- Loss: 5.2162 | F1 = 43.57 | EM = 13.47\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 43.57\n","EM = 13.47\n","\n","\n",">>> Train Epoch: [17 / 30]\n","[train-17] step: [1000 / 1400] | loss = 4.0375 | F1 = 50.77 | EM = 20.46\n","used_time: 235.88s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 17]: 0h 05m 30s <> <>\n","Training Epoch 17 -- Loss: 4.0413 | F1 = 50.30 | EM = 19.94\n","\n",">>> Dev Epoch: [17 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 17]: 0h 00m 24s <> <>\n","Validation Epoch 17 -- Loss: 5.2106 | F1 = 44.34 | EM = 13.20\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 44.34\n","EM = 13.20\n","\n","\n",">>> Train Epoch: [18 / 30]\n","[train-18] step: [1000 / 1400] | loss = 3.9495 | F1 = 50.71 | EM = 19.80\n","used_time: 235.81s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 18]: 0h 05m 31s <> <>\n","Training Epoch 18 -- Loss: 3.9882 | F1 = 50.38 | EM = 19.57\n","\n",">>> Dev Epoch: [18 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 18]: 0h 00m 25s <> <>\n","Validation Epoch 18 -- Loss: 5.1422 | F1 = 43.77 | EM = 13.57\n","\n",">>> Train Epoch: [19 / 30]\n","[train-19] step: [1000 / 1400] | loss = 3.9375 | F1 = 50.33 | EM = 20.16\n","used_time: 234.44s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 19]: 0h 05m 28s <> <>\n","Training Epoch 19 -- Loss: 3.9291 | F1 = 50.62 | EM = 20.54\n","\n",">>> Dev Epoch: [19 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 19]: 0h 00m 25s <> <>\n","Validation Epoch 19 -- Loss: 5.1793 | F1 = 44.06 | EM = 14.33\n","Epoch    18: reducing learning rate of group 0 to 6.2500e-05.\n","\n",">>> Train Epoch: [20 / 30]\n","[train-20] step: [1000 / 1400] | loss = 3.8218 | F1 = 51.40 | EM = 21.08\n","used_time: 237.62s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 20]: 0h 05m 32s <> <>\n","Training Epoch 20 -- Loss: 3.8270 | F1 = 51.15 | EM = 20.77\n","\n",">>> Dev Epoch: [20 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 20]: 0h 00m 25s <> <>\n","Validation Epoch 20 -- Loss: 5.2723 | F1 = 44.38 | EM = 13.63\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 44.38\n","EM = 13.63\n","\n","\n",">>> Train Epoch: [21 / 30]\n","[train-21] step: [1000 / 1400] | loss = 3.8062 | F1 = 51.25 | EM = 21.12\n","used_time: 237.67s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 21]: 0h 05m 34s <> <>\n","Training Epoch 21 -- Loss: 3.8086 | F1 = 51.16 | EM = 20.93\n","\n",">>> Dev Epoch: [21 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 21]: 0h 00m 24s <> <>\n","Validation Epoch 21 -- Loss: 5.2640 | F1 = 44.19 | EM = 14.00\n","\n",">>> Train Epoch: [22 / 30]\n","[train-22] step: [1000 / 1400] | loss = 3.7579 | F1 = 51.59 | EM = 20.54\n","used_time: 237.71s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 22]: 0h 05m 31s <> <>\n","Training Epoch 22 -- Loss: 3.7577 | F1 = 51.30 | EM = 20.37\n","\n",">>> Dev Epoch: [22 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 22]: 0h 00m 24s <> <>\n","Validation Epoch 22 -- Loss: 5.3103 | F1 = 44.03 | EM = 13.50\n","Epoch    21: reducing learning rate of group 0 to 3.1250e-05.\n","\n",">>> Train Epoch: [23 / 30]\n","[train-23] step: [1000 / 1400] | loss = 3.7233 | F1 = 51.62 | EM = 21.12\n","used_time: 237.91s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 23]: 0h 05m 31s <> <>\n","Training Epoch 23 -- Loss: 3.6917 | F1 = 51.64 | EM = 21.16\n","\n",">>> Dev Epoch: [23 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 23]: 0h 00m 25s <> <>\n","Validation Epoch 23 -- Loss: 5.3343 | F1 = 44.35 | EM = 13.80\n","\n",">>> Train Epoch: [24 / 30]\n","[train-24] step: [1000 / 1400] | loss = 3.6819 | F1 = 51.67 | EM = 20.80\n","used_time: 236.08s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 24]: 0h 05m 30s <> <>\n","Training Epoch 24 -- Loss: 3.6856 | F1 = 51.78 | EM = 21.14\n","\n",">>> Dev Epoch: [24 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 24]: 0h 00m 25s <> <>\n","Validation Epoch 24 -- Loss: 5.3092 | F1 = 44.12 | EM = 13.47\n","Epoch    23: reducing learning rate of group 0 to 1.5625e-05.\n","\n",">>> Train Epoch: [25 / 30]\n","[train-25] step: [1000 / 1400] | loss = 3.6836 | F1 = 51.57 | EM = 21.68\n","used_time: 237.40s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 25]: 0h 05m 30s <> <>\n","Training Epoch 25 -- Loss: 3.6609 | F1 = 51.96 | EM = 21.77\n","\n",">>> Dev Epoch: [25 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 25]: 0h 00m 25s <> <>\n","Validation Epoch 25 -- Loss: 5.3261 | F1 = 44.23 | EM = 13.63\n","\n",">>> Train Epoch: [26 / 30]\n","[train-26] step: [1000 / 1400] | loss = 3.6025 | F1 = 52.45 | EM = 22.26\n","used_time: 238.80s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 26]: 0h 05m 33s <> <>\n","Training Epoch 26 -- Loss: 3.6440 | F1 = 52.15 | EM = 21.96\n","\n",">>> Dev Epoch: [26 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 26]: 0h 00m 24s <> <>\n","Validation Epoch 26 -- Loss: 5.3154 | F1 = 44.23 | EM = 13.50\n","Epoch    25: reducing learning rate of group 0 to 7.8125e-06.\n","\n",">>> Train Epoch: [27 / 30]\n","[train-27] step: [1000 / 1400] | loss = 3.5709 | F1 = 52.34 | EM = 22.50\n","used_time: 237.65s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 27]: 0h 05m 32s <> <>\n","Training Epoch 27 -- Loss: 3.6095 | F1 = 51.98 | EM = 22.14\n","\n",">>> Dev Epoch: [27 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 27]: 0h 00m 24s <> <>\n","Validation Epoch 27 -- Loss: 5.3231 | F1 = 44.69 | EM = 13.77\n","Saved model to drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","!!! Updated: \n","F1 = 44.69\n","EM = 13.77\n","\n","\n",">>> Train Epoch: [28 / 30]\n","[train-28] step: [1000 / 1400] | loss = 3.6334 | F1 = 52.65 | EM = 22.28\n","used_time: 238.75s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 28]: 0h 05m 36s <> <>\n","Training Epoch 28 -- Loss: 3.6375 | F1 = 52.42 | EM = 21.94\n","\n",">>> Dev Epoch: [28 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 28]: 0h 00m 25s <> <>\n","Validation Epoch 28 -- Loss: 5.3110 | F1 = 44.31 | EM = 13.67\n","\n",">>> Train Epoch: [29 / 30]\n","[train-29] step: [1000 / 1400] | loss = 3.6279 | F1 = 52.46 | EM = 21.64\n","used_time: 241.96s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 29]: 0h 05m 37s <> <>\n","Training Epoch 29 -- Loss: 3.6110 | F1 = 52.39 | EM = 22.14\n","\n",">>> Dev Epoch: [29 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 29]: 0h 00m 24s <> <>\n","Validation Epoch 29 -- Loss: 5.3215 | F1 = 44.35 | EM = 13.73\n","Epoch    28: reducing learning rate of group 0 to 3.9063e-06.\n","\n",">>> Train Epoch: [30 / 30]\n","[train-30] step: [1000 / 1400] | loss = 3.5829 | F1 = 52.47 | EM = 21.94\n","used_time: 237.64s\n","<> <> Timer [Train] <> <> Interval [Training Epoch 30]: 0h 05m 33s <> <>\n","Training Epoch 30 -- Loss: 3.6085 | F1 = 52.38 | EM = 21.87\n","\n",">>> Dev Epoch: [30 / 30]\n","<> <> Timer [Train] <> <> Interval [Validation Epoch 30]: 0h 00m 24s <> <>\n","Validation Epoch 30 -- Loss: 5.3345 | F1 = 44.38 | EM = 13.93\n","<> <> <> Finished Timer [Train] <> <> <> Total time elapsed: 2h 57m 14s <> <> <>\n","Finished Training: drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n","\n","<<<<<<<<<<<<<<<< MODEL SUMMARY >>>>>>>>>>>>>>>> \n","Best epoch = 27; \n","F1 = 44.69\n","EM = 13.77\n","\n"," <<<<<<<<<<<<<<<< MODEL SUMMARY >>>>>>>>>>>>>>>> \n","Restoring best model\n","[ Loading saved model drive/MyDrive/CODE/CMRC/data_graphflow/models2/params.saved ]\n","[ Fix word embeddings ]\n","[ Using bidirectional lstm encoder ]\n","[ Using bidirectional lstm encoder ]\n","[ Using bidirectional lstm encoder ]\n","[ Using lstm encoder ]\n","[ Multi-perspective GraphLearner: 1 ]\n","[ Using 5-hop ContextGraphNN ]\n","[ Using graph type: dynamic ]\n","[ Using 5-hop ContextGraphNN ]\n","[ Using graph type: dynamic ]\n","<> <> <> Starting Timer [Test] <> <> <>\n","<> <> <> Finished Timer [Test] <> <> <> Total time elapsed: 0h 00m 26s <> <> <>\n","[test] | test_exs = 300 | step: [300 / 300] | F1 = 45.16 | EM = 14.73\n","Finished Testing: drive/MyDrive/CODE/CMRC/data_graphflow/models2/\n"],"name":"stdout"}]}]}